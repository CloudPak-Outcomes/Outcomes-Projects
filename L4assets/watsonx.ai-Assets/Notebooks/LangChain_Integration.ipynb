{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n# Use Watsonx and `LangChain` to make a series of calls to a language model"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "#### Disclaimers\n\n- Use only Projects and Spaces that are available in watsonx context.\n\n\n## Notebook content\n\nThis notebook contains the steps and code to demonstrate Simple Sequential Chain using langchain integration with Watsonx models. There are 2 sections:\n1. Using a simple sequential chain with 2 different LLMs to perform 2 generation tasks.\n2. Using a simple sequential chain to perform an extraction task followed by a classification task using the a single LLM.\n\nSome familiarity with Python is helpful. This notebook uses Python 3.10.\n\n\n## Learning goal\n\nThe goal of this notebook is to demonstrate how to chain `google/flan-ul2` and `google/flan-t5-xxl` models to generate a sequence of chains. You generate a random question on a given topic and then generate an answer to that question. You utilize the LangChain framework, using a simple chain (LLMChain) and the extended chain (SimpleSequentialChain) with WatsonxLLM.\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n- [Setup](#setup)\n- [Foundation Models in Watsonx.ai](#models)\n- [WatsonxLLM interface](#watsonxllm)\n- [Simple Sequential Chain experiment](#experiment)\n- [Summarize customer complaints](#summarize)"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a Watson Studio Project and associate a Watson Machine Learning service to the project.\n-  Download the LangChain Integration.ipynb Python Notebook to your local computer so that you can upload it to Watson Studio.  "}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Install and import the `datasets` and dependecies"}, {"metadata": {"pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "# Installing the Watson Machine Learning, Pydantic and LangChain packages\n\n!pip install \"ibm-watson-machine-learning>=1.0.320\" | tail -n 1\n!pip install \"pydantic>=1.10.0\" | tail -n 1\n!pip install langchain | tail -n 1", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** It is good practice and recommended to install all the required Python packages at the beginning of the notebook.\nHowever, to show what packages and libraries are required to execute a block of code successfully, the installation\nand importing of spcecific libraies were moved down to the cells where the library is called. \nThis was done for educational purposes and mainly for two reasons:\n- To highlight the libraries that are needed and when they are needed\n- To make the connection between the library and the code\n\nThere is also a print statement at the end of most cells. If you want to see the output of a cell, uncomment the print statement and rerun the code in the cell."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the WML credentials\nIBM watsonx.ai works with Large Langauge Models (LLMs) in the context of a project, and uses the resources and services available to the project. In the code block below, we will first attempt to retrieve the project ID from where this notebook is running. If there is an error, then you will need to provide the project ID explicitly.\n\n**Action:** Provide the IBM Cloud user API key. For details, see\n[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Input your WML API key when requested when you run the cell\n# It is good practice not no hard-code you credentials in a notebook, but let the user provide it during runtime\n\nimport getpass\n\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}\n\nurl = credentials['url']\napikey = credentials['apikey']\n\n#print(credentials)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the project id\nIBM watsonx.ai works with Large Langauge Models (LLMs) in the context of a project, and uses the resources and services available to the project. In the code block below, we will first attempt to retrieve the project ID from where this notebook is running. If there is an error, then you will need to provide the project ID explicitly."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import the os library and retrieve the project ID from the underlying operation system environment\n\nimport os\n\ntry:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")\n\n#print(project_id)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"models\"></a>\n## Foundation Models in `watsonx.ai`"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "#### List available models\n\nAll avaliable models are presented under `ModelTypes` class."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import a required WML library and list the foundation models currently hosted on the watsonx.ai platform  \n\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n\nprint([model.name for model in ModelTypes])", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "You need to specify `model_id`s to identify the LLMs you are using. In this notebook, the `flan-ul2` and `flan-t5-xxl` models are used (as shown below). You can experiement with other models. Review the cell below for correct syntax (all lowecase including company that created and trained the LLM)."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Create model ids for each model that will be used in the rest of the notebook\n\nmodel_id_1 = \"google/flan-ul2\"\nmodel_id_2 = \"google/flan-t5-xxl\"\n\n#print(model_id_1)\n#print(model_id_2)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Defining the model parameters\n\nYou might need to adjust model `parameters` for different models or tasks, to do so please refer to documentation under `GenTextParamsMetaNames` class. Note that in this part of the lab, you are using the same parameters for both the `flan-ul2` and the `flan-t5-xxl` models. You can choose to use different parameters (this will be demonstrated in the second part of this notebook below).\n\n**Action:** If any complications please refer to the [documentation](https://ibm.github.io/watson-machine-learning-sdk/)."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import the required WML libraries\n\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n\n# Specify the LLM model parameters\n\nparameters = {\n    \"decoding_method\": \"sample\",\n    \"max_new_tokens\": 100,\n    \"min_new_tokens\": 1,\n    \"temperature\": 0.5,\n    \"top_k\": 50,\n    \"top_p\": 1\n}", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"watsonxllm\"></a>\n## WatsonxLLM interface\n\n`WatsonxLLM` is a wrapper around watsonx.ai models that provide chain integration around the models.\n\n**Action:** For more details about `CustomLLM` check the [LangChain documentation](https://python.langchain.com/docs/integrations/llms/ibm_watsonx/)"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Install the langchain_ibm package that facilitates LangChain integration with IBM watsonx.ai models\n# From langchain_ibm import WatsonxLLM\n\n!pip install -U langchain-ibm\nfrom langchain_ibm import WatsonxLLM\n\n# WatsonxLLM puts a wrapper around the foundation models to make it compatible with LangChain\n\nflan_ul2_llm = WatsonxLLM(\n    model_id=model_id_1,\n    url=url,\n    project_id=project_id,\n    params=parameters,\n    apikey=apikey)\n\nflan_t5_llm = WatsonxLLM(\n    model_id=model_id_2,\n    url=url,\n    project_id=project_id,\n    params=parameters,\n    apikey=apikey)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "You can print all set data about the WatsonxLLM object using the `dict()` method. Here you are only printing it for the `flan-ul2` model. The `flan-t5-xxl` model uses the same set of parameters. "}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Print the parameters associated with the WatsonLLM object (LLM model)\n\nflan_ul2_llm.dict()", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"experiment\"></a>\n## Simple Sequential Chain experiment\n\nThe simplest type of sequential chain is called a `SimpleSequentialChain`, in which each step has a single input and output and the output of one step serves as the input for the following step.\n\nThe experiment consists of generating a random question about any topic with one LLM (`flan-ul2`) and then answering the question with the other LLM (`flan-t5-xxl`)."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "An object called `PromptTemplate` assists in generating prompts using a combination of user input, additional non-static data, and a fixed template string.\n\nIn our case we would like to create two `PromptTemplate` objects which will be responsible for creating a random question and answering it."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import the required langchain library\n\nfrom langchain.prompts import PromptTemplate\n\n# Create two prompts\n# The 1st prompt contains the structure of the 1st Question with a dynamic topic provided  by the user. This is sent to flan-ul2 to generate a completion. \n# The 2nd prompt contains the structure of the 2nd Question with dynamic question (generated by flan-ul2). This is sent to flan-t5-xxl to generate a final answer.  \n\ntemplate_1 = \"Generate a random question about {topic}: Question: \"\nprompt_1 = PromptTemplate.from_template(template_1)\n\ntemplate_2 =\"Answer the following question: {question}\"\nprompt_2 = PromptTemplate.from_template(template_2)\n\n#print(prompt_1)\n#print(prompt_2)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "We would like to add functionality around language models using `LLMChain` chain (deprecated). The deprecated syntax as well as the new \nand alternative syntaxes on how to construct a chain (also called a runnable sequence) ia given in the cells below. The new syntax uses \nLCEL (LangChain expression language), a declarative way to easily compose chains together. The core functionality however did not change. \n\nThe `prompt_to_flan_ul2` chain formats the prompt template whose task is to take the user input and then pass the formatted string to the first LLM (`flan-ul2`) and then returns the LLM's output."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import the required LangChain library\n\n#from langchain.chains import LLMChain\nfrom langchain_core.runnables import RunnableSequence\n\n# Create a simple chain consisting of the 1st prompt and the 1st LLM\n\n#prompt_to_flan_ul2 = LLMChain(prompt=prompt_1, llm=flan_ul2_llm) - deprecated syntax\n#prompt_to_flan_ul2 = prompt_1 | flan_ul2_llm - alternative new syntax\nprompt_to_flan_ul2 = RunnableSequence(first=prompt_1, last=flan_ul2_llm)\n\n#print(prompt_to_flan_ul2)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "The `ul2_to_t5` chain formats the prompt template whose task is the take the question generated by the `prompt_to_flan_ul2` chain, passes the formated string to the 2nd LLM (`flan-t5-xxl`), and returns its output."}, {"metadata": {"pycharm": {"name": "#%%\n"}, "scrolled": true}, "cell_type": "code", "source": "# Create a 2nd simple chain consisting of the 2nd prompt and the 2nd LLM \n\n#ul2_to_t5 = LLMChain(prompt=prompt_2, llm=flan_t5_llm) - deprecated syntax\n#ul2_to_t5 = prompt_2 | flan_t5_llm - alternative new syntax\nul2_to_t5 = RunnableSequence(first=prompt_2, last=flan_t5_llm)\n\n#print(ul2_to_t5)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "This is the overall chain where we run `prompt_to_flan_ul2` and `ul2_to_t5` chains in sequence."}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Generate random question and answer to topic."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "# Import the callback handler that prints chain components' inputs and outputs to the console\n# This is very helpful to verify that the chain is working as intended\n\nfrom langchain.callbacks.tracers import ConsoleCallbackHandler", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Providing the chained output. You pass in the word \"Australia\" and prompt_to_flan_ul2 will generate a random question on Australia. \n# This question is then passed to ul2_to_t5 to get an answer to that random question.\n\nqa = RunnableSequence(first=prompt_to_flan_ul2, last=ul2_to_t5)\n\nchain_output = qa.invoke(\"Australia\", config={'callbacks': [ConsoleCallbackHandler()]})\n\n#print(chain_output)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The example above is a simple sequential chain where the output of one LLM is passed as the input to a second LLM. Note that there is no absolute need for 2 different LLMs. The example below shows a more complex sequnetial chain where the same LLM (flan-ul2 in this example) is used for 2 different use cases.\n\n1. In the first part of the chain, the flan-ul2 model is used to extract information from the input.\n2. In the second part of the chain, the extracted information is passed to flan-ul2 again to perform a classisifcation task.\n\nIn addition, you will use different parameters setting for the first extraction task (with sampling decoding and a max_new_tokens setting of 100) from the second classification task (this uses greedy decoding with a max_new_tokes of 10 only). In both cases, you are using the flan-ul2 model. "}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summarize\"></a>\n## Summarize and classify customer complaints chain"}, {"metadata": {}, "cell_type": "code", "source": "# We are reusing variables that were declared earlier, but we're creating different model types\n# In the example FLAN_U2 is used in both instances, but the models may be different\n# We have different sets of parameters for each model as the yes or no classification needs very few tokens\n\nmodel_id_1 = \"google/flan-ul2\"\nmodel_id_2 = \"google/flan-ul2\" \n\nmodel1_parameters = {\n    \"decoding_method\": \"sample\",\n    \"max_new_tokens\": 100,\n    \"min_new_tokens\": 1,\n    \"top_k\": 50,\n    \"top_p\": 1    \n}\n\nmodel2_parameters = {\n    \"decoding_method\": \"greedy\",\n    \"max_new_tokens\": 10,\n    \"min_new_tokens\": 1,\n    \"top_k\": 50,\n    \"top_p\": 1       \n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you have done in the earlier example, you use WatsonxLLM to provide a wrapper around foundation models. Note that the name clearly identifies the purpose of each wrapper."}, {"metadata": {}, "cell_type": "code", "source": "# WatsonxLLM puts a wrapper around the foundation models to make it compatible with LangChain\n\nflan_ul2_llm_extract = WatsonxLLM(\n    model_id=model_id_1,\n    url=url,\n    project_id=project_id,\n    params=parameters,\n    apikey=apikey)\n\nflan_ul2_llm_classify = WatsonxLLM(\n    model_id=model_id_2,\n    url=url,\n    project_id=project_id,\n    params=parameters,\n    apikey=apikey)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Use for testing/debugging\n# flan_ul2_llm_extract.dict()\n# flan_ul2_llm_classify.dict()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The first part of the chain asks `flan-ul2` to analyze the user-provided prompt text to extract 3 complaints. The second part of the chain again uses `flan-ul2` to classify the 3 complaints and classify them to see if the complaints involve identity theft."}, {"metadata": {}, "cell_type": "code", "source": "# Create prompt templates for summarizing and classifying customer complaints\n\ntemplate_1 = \"From the following customer complaint, extract 3 factors that caused the customer to be unhappy. Put each factor on a new line. Customer complaint: {customer_complaint}. Numbered list of complaints: 1.\"\nprompt_1 = PromptTemplate.from_template(template_1)\n\ntemplate_2 =\"Does the following statements contain the concept of identify theft?: {list_of_complaints}\"\nprompt_2 = PromptTemplate.from_template(template_2)\n\n#print(prompt_1)\n#print(prompt_2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Create the LLM chains: model + prompt\n\n#get_list_of_complaints = LLMChain(llm=flan_ul2_llm_extract, prompt=prompt_1) - deprecated syntax\nget_list_of_complaints = RunnableSequence(first=prompt_1, last=flan_ul2_llm_extract)\n#check_for_identify_theft = LLMChain(llm=flan_ul2_llm_classify, prompt=prompt_2) - deprecated syntax\ncheck_for_identify_theft = RunnableSequence(first=prompt_2, last=flan_ul2_llm_classify)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Creating the sequential chain."}, {"metadata": {}, "cell_type": "code", "source": "# Sequential chain:\n\n#customer_complaint_chain = SimpleSequentialChain(chains=[get_list_of_complaints, check_for_identify_theft], verbose=True)\ncustomer_complaint_chain = RunnableSequence(first=get_list_of_complaints, last=check_for_identify_theft)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The customer_complaint is the input being used for this exercise. You can modify this text to test the notebook. This cell block will generate the classification."}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# You can experiment with different values of customer complaints\n\ncustomer_complaint = \"I am writing you this statement to delete the following information on my credit report. \\\n                     The items I need deleted are listed in the report. I am a victim of identity thief, I demand that you \\\n                     remove these errors to correct my report immediately! I have reported this to the federal trade commission \\\n                     and have attached the federal trade commission affidavit. Now that I have given you the following information, \\\n                     please correct my credit report or I shall proceed with involving my attorney!\"\n\nchain_output = customer_complaint_chain.invoke(customer_complaint, config={'callbacks': [ConsoleCallbackHandler()]})", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "This cell block adds the question to the output so that it can be printed."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#print(chain_output)\nprint(\"Is this customer reporting identity theft: \" + chain_output)\n\n# A production application will contain logic to trigger some action if identity theft is true", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Authors: \n **Mateusz Szewczyk**, Software Engineer at Watson Machine Learning.\n **Elena Lowery**, Data and AI Architect.\n \n Notebook updated by **Andre de Waal**, Senior Learning Content Developer.\n "}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "Copyright \u00a9 2024 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}