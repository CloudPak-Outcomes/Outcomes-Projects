{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## LangChain memory with watsonx.ai models\n\nThis notebook contains sample code for using *ConversationBufferMemory* with models included in watsonx.ai\n"}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Install dependecies"}, {"metadata": {"pycharm": {"is_executing": true, "name": "#%%\n"}}, "cell_type": "code", "source": "!pip install pip install ibm-watson-machine-learning --upgrade\n!pip install langchain | tail -n 1\n!pip install langchain-community\n!pip install -U langchain-ibm", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Define the WML credentials\nThis cell defines the WML credentials required to work with watsonx Foundation Model inferencing.\n\n**Action:** Provide the IBM Cloud user API key. For details, see\n[documentation](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui)."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "import getpass\n\ncredentials = {\n    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n    \"apikey\": getpass.getpass(\"Please enter your WML api key (hit enter): \")\n}\n\nurl = credentials['url']\napikey = credentials['apikey']", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Define the project id\nThe Foundation Model requires project id that provides the context for the call. We will obtain the id from the project in which this notebook runs. Otherwise, please provide the project id."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "import os\n\ntry:\n    project_id = os.environ[\"PROJECT_ID\"]\nexcept KeyError:\n    project_id = input(\"Please enter your project_id (hit enter): \")", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Import required packages"}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\nfrom ibm_watson_machine_learning.foundation_models.utils.enums import DecodingMethods\n#from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\nfrom langchain_ibm import WatsonxLLM\n\n\nfrom langchain import PromptTemplate\nfrom langchain.chains import LLMChain\nfrom langchain.chains import SimpleSequentialChain\n\nfrom langchain.chains import ConversationChain\nfrom langchain.memory import ConversationBufferMemory\n\nfrom langchain.llms.utils import enforce_stop_tokens", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# If you want to change model types, you can look them up with this line\nprint([model.name for model in ModelTypes])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Create the LangChain model object"}, {"metadata": {}, "cell_type": "code", "source": "# We will use the llama model\n\n# Experiment with different models. We noticed that Flan produces a more concise ouput, but llama - more descriptive and better quality#model_id_1 = \"meta-llama/llama-2-70b-chat\"\n#model_id_1 = \"google/flan-ul2\"\nmodel_id_1 = \"meta-llama/llama-2-13b-chat\"\n\nmodel1_parameters = {\n    \"decoding_method\": \"greedy\",\n    \"max_new_tokens\": 300,\n    \"min_new_tokens\": 1,\n    \"top_k\": 50,\n    \"top_p\": 1\n}\n\n# WML model objects - will be used to create models for the LangChain API\n# Create the model objects that will be used by LangChain\n\ncurrent_llm = WatsonxLLM(\n    model_id=model_id_1,\n    url=url,\n    project_id=project_id,\n    params=model1_parameters,\n    apikey=apikey)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Create the memory object\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=current_llm, \n    memory = memory,\n    verbose=True\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Uncomment and run these lines if you want to clear the memory without stopping the notebook (use if you provide different user input in the cell below)\nmemory.clear()\n#print(memory.buffer)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Example of a first quesiton from the user\nuser_input = \"From the following customer complaint, extract 3 factors that caused the customer to be unhappy. \\\n                            Put each factor on a new line. Customer complaint: I am writing you this statement to delete the \\\n                            following information on my credit report. The items I need deleted are listed in the report. \\\n                            I am a victim of identity thief, I demand that you remove these errors to correct my report immediately! \\\n                            I have reported this to the federal trade commission and have attached the federal trade commission affidavit. \\\n                            Now that I have given you the following information, please correct my credit report or I shall proceed with involving my attorney! \\\n                            Numbered list of complaints:\"\n\n#user_input=\"Right now I am bothered! I have attempted to be patient however it is hard to be patient when you feel that you are continually being overlooked by somebody. I think you fail to remember that \\Consumer detailing organizations have expected an essential part in amassing and assessing customer credit and other data on shoppers. The XXXX XXXX  is reliant on the reasonable and precision.\"\n\n# Invoke the LLM\nconversation.predict(input=user_input)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Example of a second quesiton from the user\nuser_input = \"Does the numbered list of complaints contain a statement about identity fraud? Provide a short answer: yes or no.\"\nconversation.predict(input=user_input)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# For debugging, print the history of the conversation\nprint(memory.buffer)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "### Authors: \n **Elena Lowery**, Data and AI Architect\n### Updated by:\n**Andre de Waal**, Learning Content Developer"}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}