{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"collapsed":true},"source":["# Finding accident clusters\n","\n","## CPDaaS: Make sure to first insert a \"project token\"\n","Click on the three vertical dots icon in the uper right of the screen, then click on Insert project token\n","\n","**Once inserted, execute the cell**.\n","\n","A project token is only available if you followed the prerequesite instructions to create one in your project."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import warnings\n","import pandas as pd\n","import numpy as np\n","import os\n","from ibm_watson_studio_lib import access_project_or_space\n","\n","import matplotlib.pyplot as plt\n","# matplotlib.patches lets us create colored patches, which we can use for legends in plots\n","import matplotlib.patches as mpatches\n","\n","%matplotlib inline\n","\n","# Get access to the prohject API for CPD on-premises\n","if \"USER_ID\" in os.environ :\n","    wslib = access_project_or_space()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Install folium for map rendering\n","!pip install folium 2>&1 >foliumpip.out\n","\n","import folium"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Read the ChicagoCrashes.csv file\n","The Chicago crashes were collected in a previous exercise and stored in a file in the project \n","after being reduces to the cleansed data required."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["body = wslib.load_data(\"ChicagoCrashes.csv\")\n","crashes_df = pd.read_csv(body)\n","crashes_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Divide dataset into accident categories: fatal, non-fatal but with injuries, none of the above\n","We'll need different sets of data based on if there are injuries or not."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["killed_df = crashes_df[crashes_df['injuries_fatal']>0]\n","injured_df = crashes_df[np.logical_and(crashes_df['injuries_total']>0, crashes_df['injuries_fatal']==0)]\n","# killed_or_injured_df = killed_df.append(injured_df) # \"append\" has been deprecated in favor of concat\n","killed_or_injured_df = pd.concat([killed_df, injured_df], ignore_index=True)\n","nothing_df = crashes_df[np.logical_and(crashes_df['injuries_fatal']==0, crashes_df['injuries_total']==0)]\n","\n","print(\"Number of records: {}\".format(crashes_df.shape[0]))\n","print(\"Number of fatal accidents: {}\".format(killed_df.shape[0]))\n","print(\"Number of injury accidents: {}\".format(injured_df.shape[0]))\n","print(\"Number of no-injury accidents: {}\".format(nothing_df.shape[0]))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Finding clusters of accidents\n","There are multiple ways to cluster data based on similarities. \n","This notebook limits itself to trying the DBSCAN and the Agglomerative Clustering algorithms.\n","This demonstrates the need to try multiple methods before deciding on a final solution.\n","\n","For more information on clustering, see:\n","- Byte-Size Data Science Youtube videos: \n","  - <a href=\"https://youtu.be/MhtuJfYNYdo\" target=\"_blank\">76 clustering video</a>\n","  - <a href=\"https://youtu.be/3k68cUIUuqs\" target=\"_blank\">77 DBSCAN video</a>\n","- Byte-Size Data Science accompanying Notebooks: \n","  - <a href=\"https://github.com/jacquesroy/byte-size-data-science/blob/master/Notebooks/076-Clustering.ipynb\" target=\"_blank\">076 Clustering</a>\n","  - <a href=\"https://github.com/jacquesroy/byte-size-data-science/blob/master/Notebooks/077-DBSCAN.ipynb\" target=\"_blank\">077 DBSCAN</a>\n","- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\" target=\"_blank\">sklearn.cluster.DBSCAN</a>\n","- <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering\" target=\"_blank\">sklearn.cluster.AgglomerativeClustering</a>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Find the clusters with DBSCAN\n","DBSCAN `eps` parameter is used to identify maximum distance between two samples for one to be considered \n","as in the neighborhood of the other.\n","\n","Distances:\n","In the Chicago area, the value 0.00015 represents roughly:\n","- Horizontal (longitudinal) distance: 40 feet\n","- Vertical (latitudinal) distance: 54 feet\n","- Diagonal distance: 68 feet"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.cluster import DBSCAN\n","from sklearn import metrics\n","from sklearn.datasets import make_blobs\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# The eps value of 0.0015 was chosen after multiple trials\n","# Trying multiple parameter values is essential in finding the best solution \n","db = DBSCAN(eps=0.0015, min_samples=50).fit(crashes_df[['latitude','longitude']])\n","\n","core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n","core_samples_mask[db.core_sample_indices_] = True\n","labels = db.labels_\n","\n","# Number of clusters in labels, ignoring noise if present.\n","n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n","n_noise_ = list(labels).count(-1)\n","\n","print('Estimated number of clusters: %d' % n_clusters_)\n","print('Estimated number of noise points: %d' % n_noise_)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Display the cluster centers on a map"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df = crashes_df[['latitude','longitude']].copy(deep=True)\n","results_df['cluster'] = db.labels_\n","results_df = results_df[results_df['cluster'] > -1].reset_index(drop=True) # remove noise points\n","results_df = results_df.groupby('cluster').agg(latitude=pd.NamedAgg(column='latitude',aggfunc=\"mean\"), \n","                                               longitude = pd.NamedAgg(column='longitude',aggfunc=\"mean\"), \n","                                               cnt = pd.NamedAgg(column='cluster', aggfunc='count') ).\\\n","                                           reset_index().sort_values('cnt', ascending=False).reset_index(drop=True)\n","results_df['cluster'] = results_df.index # lower index bigger cluster\n","minval=results_df['cnt'].min()\n","maxval= 1 + results_df['cnt'].max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use a colormap to colorcode the count of accidents in each cluster\n","# It's difficult to find a good colormap to use.\n","# see: https://matplotlib.org/3.3.3/gallery/color/colormap_reference.html\n","import matplotlib.cm as cm\n","\n","colors = cm.get_cmap('viridis', maxval)(range(maxval),bytes=True)\n","\n","rgbcolors = []\n","for v in colors :\n","    col = np.floor(v * 255)\n","    r = int(col[0])\n","    g = int(col[1])\n","    b = int(col[2])\n","    rgbcolors.append('#' + '{0:#08x}'.format(((r * 65536) + (g * 256) + b))[2:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display the average center of each group\n","latlong = results_df[['latitude','longitude']].mean(axis=0) # To center the map\n","chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"90%\", height=\"90%\")\n","\n","# Since the dataframe is ordered, we can display the top 20 only\n","for idx, coord in results_df[0:20].iterrows():\n","    tooltip_content=\"Cluster: {0}, count: {1}\".format(coord['cluster'].astype(int),coord['cnt'].astype(int) )\n","    folium.Circle(radius=500,\n","                  location=[coord['latitude'], coord['longitude']],\n","                  # popup=row.hgroup,\n","                  color=rgbcolors[coord['cnt'].astype(int) ],\n","                  tooltip=tooltip_content,\n","                  fill=True,\n","                  fill_color=rgbcolors[coord['cnt'].astype(int) ]\n","    ).add_to(chi_map)\n","    \n","chi_map"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## DBSCAN conclusion\n","The DBSCAN algorithm requires a lot of tuning to arrive at a desired solution. The results are difficult to evaluate.\n","\n","In this notebook example, there are 66 clusters and 44914 accidents were dismissed as noise. \n","Different parameter values results in different number of clusters and noise values. \n","The top three clusters are close to each other, around downtowm Chicago. \n","You can see this by zooming in the map and hover the cursor over the cluster centers. \n","The cluster numbers represent theorder of the clusters.\n","\n","It may be possible to figure out a way to group clusters together to get to a top-5 list. \n","Instead, we'lll look at a different approach."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Find the clusters with hierarchical\n","for the following algorithm, using all the accidents (51,272) is too much for the resources available in the notebook. \n","The notebook restarts the kernel. I assume it runs out of resources.\n","\n","Instead, we use the accidents with injuries (fatal or not). Around 7,200 records. \n","The reasoning is that these accidents are more significant and should provide more significant clusters.\n","\n","If you run out of resources in the next step, change your runtime to a larger one such as: `Runtime 22.2.on Python 3.10 XS`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import objects assuming the k-means section was skipped\n","from scipy import ndimage \n","from scipy.cluster import hierarchy \n","from scipy.cluster.hierarchy import dendrogram\n","from scipy.spatial import distance_matrix \n","from sklearn import manifold \n","from sklearn.cluster import AgglomerativeClustering \n","#from sklearn.datasets.samples_generator import make_blobs\n","from sklearn.datasets import make_blobs"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### First pass: get the hierarchy\n","The first step is to see how the hierarchy is put together.\n","The result is seen in a dendrogram.\n","\n","See also:\n","- <a href=\"https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\" target=\"_blank\">Plot Hierarchical Clustering Dendrogram</a>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ac = AgglomerativeClustering(n_clusters=None,distance_threshold=0)\n","clusters = ac.fit(killed_or_injured_df[['longitude','latitude']])\n","print(\"n_clusters_: {}\\nn_leaves_: {}\".format(clusters.n_clusters_, clusters.n_leaves_))\n","print(\"n_connected_components_: {}\\nn_features_in_: {}\".format(clusters.n_connected_components_, clusters.n_features_in_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Utility function, courtesy of Robert Uleman from IBM\n","def get_linkage_matrix(model, **kwargs):\n","    # Create the counts of samples under each node\n","    counts = np.zeros(model.children_.shape[0])\n","    n_samples = len(model.labels_)\n","    for i, merge in enumerate(model.children_):\n","        current_count = 0\n","        for child_idx in merge:\n","            if child_idx < n_samples:\n","                current_count += 1  # leaf node\n","            else:\n","                current_count += counts[child_idx - n_samples]\n","        counts[i] = current_count\n","\n","    linkage_matrix = np.column_stack([model.children_, model.distances_,\n","                                      counts]).astype(float)\n","    return(linkage_matrix)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["linkage_matrix = get_linkage_matrix(clusters)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Display the hierarchy"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(9, 9))  \n","ret = dendrogram(linkage_matrix, truncate_mode='lastp', p=50, no_plot=False, orientation='left')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Comments on the hierarchy\n","The hierarchy shows how smaller clusters aggregate into larger ones.\n","If we use a vertical line at any point in the hierarchy, we can see how many clusters would be required.\n","It appears at around the horizontal value of 3, we can get exactly 5 clusters.\n","\n","Since we decided that we wanted five hotspots, it fits our needs."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Second pass: Get 5 clusters\n","Earlier, we decided to use 5 hotspots. The following cells retirve the five clusters and display them on a map.\n","\n","The visual result allows us to confirm that the clusters are appropriate."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ac = AgglomerativeClustering(n_clusters=5,distance_threshold=None)\n","clusters = ac.fit(killed_or_injured_df[['longitude','latitude']])\n","print(\"n_clusters_: {}\\nn_leaves_: {}\".format(clusters.n_clusters_, clusters.n_leaves_))\n","print(\"n_connected_components_: {}\\nn_features_in_: {}\".format(clusters.n_connected_components_, clusters.n_features_in_))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Add the labels to the accidents and aggregate by cluster\n","results_df = killed_or_injured_df[['latitude','longitude']].copy(deep=True)\n","results_df['cluster'] = clusters.labels_\n","results_df = results_df[results_df['cluster'] > -1].reset_index(drop=True) # remove noise points\n","results_df = results_df.groupby('cluster').agg(latitude=pd.NamedAgg(column='latitude',aggfunc=\"mean\"), \n","                                               longitude = pd.NamedAgg(column='longitude',aggfunc=\"mean\"), \n","                                               cnt = pd.NamedAgg(column='cluster', aggfunc='count') ).\\\n","                                           reset_index().sort_values('cnt', ascending=False).reset_index(drop=True)\n","results_df['cluster'] = results_df.index # lower index bigger cluster\n","minval=results_df['cnt'].min()\n","maxval= 1 + results_df['cnt'].max()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.cm as cm\n","\n","colors = cm.get_cmap('viridis', maxval)(range(maxval),bytes=True)\n","\n","rgbcolors = []\n","for v in colors :\n","    col = np.floor(v * 255)\n","    r = int(col[0])\n","    g = int(col[1])\n","    b = int(col[2])\n","    rgbcolors.append('#' + '{0:#08x}'.format(((r * 65536) + (g * 256) + b))[2:])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Display the average center of each group\n","latlong = results_df[['latitude','longitude']].mean(axis=0) # To center the map\n","chi_map = folium.Map(location=[latlong[0], latlong[1]], zoom_start=10, width=\"90%\", height=\"90%\")\n","\n","# The dataframe is ordered\n","for idx, coord in results_df.iterrows():\n","    tooltip_content=\"Cluster: {0}, count: {1}\".format(coord['cluster'].astype(int),coord['cnt'].astype(int) )\n","    folium.Circle(radius=500,\n","                  location=[coord['latitude'], coord['longitude']],\n","                  # popup=row.hgroup,\n","                  color=rgbcolors[coord['cnt'].astype(int) ],\n","                  tooltip=tooltip_content,\n","                  fill=True,\n","                  fill_color=rgbcolors[coord['cnt'].astype(int) ]\n","    ).add_to(chi_map)\n","    \n","chi_map"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Hover over the results\n","If you hover your cursor over a cluster center, you can see the cluster number and the nuber of accidents attached to it."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Show the cluster information\n","# We can se the balance in the clusters by looking at the cnt column\n","results_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Save the cluster information to a file\n","If you've spent too long through the notebook, the `wslib.upload` operation may fail due to the expiration of the token.\n","To refresh the connection, go back up and execute the cell just before \"**Read the ChicagoCrashes.csv file**\". \n","\n","The cell ends with: `wslib = access_project_or_space(params)`\n","\n","This will retrieve a new token and re-create the wslib client. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["results_df.to_csv(\"ClusterRecords.csv\", index=False)\n","res = wslib.upload_file('ClusterRecords.csv')\n","print(\"File {} uploaded\".format(res['name']))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Clustering conclusion\n","Many projects may require more thant straightforward supervised learning models.\n","\n","This notebook demonstrate how \"full-code\" can be used to work with open-source algorithms to get to a solution. \n","It does not pretend to have gotten the optimal solution, just a possible one that appears promising.\n","\n","It also shows the difficulty of choosing an algorithm and evaluating the results. Data science is as much of an art as it is a science. It relies on the expertise and experience of data scientists."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Author\n","Jacques Roy is a member of the IBM Enablement for Data and AI\n","\n","Copyright © 2023. This notebook and its source code are released under the terms of the MIT License."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":1}
