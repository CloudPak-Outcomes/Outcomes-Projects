{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Labs\n",
    "## Introduction \n",
    "Welcome to the Spark Labs Lab Guide! In this lab, you will walk through a hands-on scenario where you will learn how to leverage Apache Spark to:\n",
    "\n",
    "- Load data efficiently\n",
    "- Save data into a table\n",
    "- Clean and preprocess data using Spark's parallel processing capabilities\n",
    "- Train a simple Logistic Regression classification model on the processed data\n",
    "\n",
    "### Highlighting Spark Labs Benefits:\n",
    "Before you dive into the lab, take a moment to see the benefits of using Spark Labs with watsonx.data, especially in comparison to using external notebooks that connects to watsonx.data.\n",
    "\n",
    "\n",
    "1.  No Dependency Management:\n",
    "    - External Notebooks: When working in an external notebook, you would typically run your code with the dependencies installed on your own computer. This can lead to version mismatches, dependency conflicts, or other setup challenges.\n",
    "    - watsonx.data: Since Spark Labs runs directly within the watsonx.data environment, you don’t need to worry about managing or troubleshooting dependencies. All the necessary packages and configurations are preconfigured on the server, ensuring consistency and saving you time.\n",
    "  \n",
    "\n",
    "2.  Easier Configuration:\n",
    "    - External Notebooks: Setting up an external environment often requires manual configuration, from installing the right versions of Spark and libraries to setting up paths and environment variables.\n",
    "    - watsonx.data: With watsonx.data, the environment is preconfigured for you. You can jump straight into coding without spending time on setup, making the process much faster and smoother.\n",
    "  \n",
    "\n",
    "3.  Integrated Development Environment:\n",
    "    - External Notebooks: Using external notebooks can require managing different tools and platforms to run Spark, track results, and visualize data, which can be disjointed and time-consuming.\n",
    "    - watsonx.data: In this environment, everything you need is integrated into a single platform. You can manage your notebooks, run Spark jobs, and view results all in real time, without switching between different tools.\n",
    "\n",
    "\n",
    "4. Easy Collaboration:\n",
    "   - External Notebooks: Collaboration can be cumbersome when working in local environments. Sharing notebooks may involve version control, syncing, or exporting and sending files manually.\n",
    "   - watsonx.data: With notebooks stored directly on the server, collaboration is seamless. You can easily share your work with colleagues and stakeholders, ensuring that everyone is on the same page and able to provide feedback in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session Setup \n",
    "\n",
    "### The Traditional Setup Method (External Notebooks)\n",
    "\n",
    "Before you can start using Apache Spark, you need to set up a Spark Session. This session is crucial as it acts as the entry point for all Spark functionality, allowing you to interact with Spark’s powerful distributed computing capabilities.\n",
    "\n",
    "In external environments, the setup typically involves configuring several options like the application name, cluster manager, memory allocation, and more. For example:\n",
    "```python\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"ComplexSparkSessionSetup\") \\\n",
    "    .setMaster(\"yarn\") \\  # Using YARN as the cluster manager\n",
    "    .set(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\  # Custom warehouse location\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"500\") \\  # Increase shuffle partitions for performance\n",
    "    .set(\"spark.executor.memory\", \"8g\") \\  # Allocate 8GB of memory to each executor\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\  # Allocate 4GB of memory to the driver\n",
    "    .set(\"spark.executor.cores\", \"4\") \\  # 4 cores per executor\n",
    "    .set(\"spark.sql.parquet.compression.codec\", \"snappy\") \\  # Use Snappy compression for Parquet files\n",
    "    .set(\"spark.ui.port\", \"4041\")  # Change the Spark UI port to avoid conflicts\n",
    "```\n",
    "\n",
    "However, when running within watsonx.data, this extensive configuration is unnecessary because it is automatically setup, streamlining the setup processes, saving time and avoiding the need to sift through documentation.\n",
    "\n",
    "### Simplified Setup for Watsonx.data (Spark Labs)\n",
    "\n",
    "In this current lab, the only configuration needed is the `fs.s3a.path.style.access` flag, which should be set to `true`. This is required because you are working with a `MinIO S3-compatible object storage bucket`, and this flag ensures that Spark can properly access it.\n",
    "\n",
    "#### Additional Useful Flags:\n",
    "There are a few other configuration options that can help fine-tune your Spark session, especially for logging:\n",
    "\n",
    "- `ae.spark.driver.log.level`: Sets the log level for the driver (e.g. `\"INFO\"`)\n",
    "- `ae.spark.executor.log.level`: Sets the log level for the executor (e.g. `\"INFO\"`)\n",
    "\n",
    "These flags are optional but can be helpful for debugging or performance tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder.appName('sparky').getOrCreate()\n",
    "conf=spark.sparkContext.getConf()\n",
    "spark.stop()\n",
    "\n",
    "conf.setAll([\n",
    "    (\"fs.s3a.path.style.access\", \"true\"),\n",
    "    # (\"ae.spark.driver.log.level\", \"INFO\"),\n",
    "    # (\"ae.spark.executor.log.level\", \"INFO\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that spark session and see the current version of spark, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing the Database and Creating the Schema\n",
    "\n",
    "Watsonx.data follows a cascading naming scheme, structured in a hierarchical form: `Catalog` > `Schema` > `Table`. This topology helps organize the data in a way that is intuitive and easy to navigate.\n",
    "\n",
    "- `Catalog`: The top level in the architecture. Each catalog contains multiple Schemas. It acts as a container for related datasets.\n",
    "- `Schema`: The middle level. A Schema organizes multiple Tables within a catalog. It serves as a namespace for tables, often reflecting logical groupings of data.\n",
    "- `Table`: The bottom level, where actual data resides. Each Table follows a specific schema definition and contains the data, often in a structured format such as rows and columns.\n",
    "\n",
    "### Working with the Hierachy Naming Scheme\n",
    "\n",
    "In Watsonx.data, the hierarchy of `Catalogs`, `Schemas`, and `Tables` is accessed using `dot notation`. This makes it easy to specify and query data at various levels of the hierarchy.\n",
    "\n",
    "For example, if you wanted to run queries on the Table `exampleTable` within the Schema `exampleSchema`, which is contained in the Catalog `exampleCatalog`, you would refer to it using the following syntax:\n",
    "```python\n",
    "exampleCatalog.exampleSchema.exampleTable\n",
    "```\n",
    "\n",
    "### Viewing Catalogs\n",
    "To view the available `Catalogs` in Watsonx.data, you can use the following approach below.\n",
    "\n",
    "Note that `spark_catalog` is the default catalog that comes with Spark. This manages the metadata of your session's databases and tables, including those stored in Spark's internal Hive-like catalog or other data sources that Spark supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW CATALOGS\n",
    "\"\"\").show()\n",
    "\n",
    "# Note: the .show() method displays outputs in a human readable table format as seen below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Schemas within a Catalog\n",
    "To view the `Schemas` within a catalog, you can use the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW SCHEMAS IN iceberg_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Schema\n",
    "Before you proceed with data ingestion, you need to create a new `Schema` where the newly created `Table` will reside.\n",
    "\n",
    "In this case, you will create a `Schema` called `demo` within the `Catalog` `iceberg_data`. Using dot notation, the full reference to this Schema is `iceberg_data.demo`.\n",
    "\n",
    "To create the new `Schema`, run the SQL command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS iceberg_data.demo\n",
    "\"\"\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will double check to see if the `Schema` was created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW SCHEMAS IN iceberg_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is now a schema called `demo` in the returned table, so the Schema creation was successful and you can move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion\n",
    "\n",
    "Now that you have the needed `Catalog` and `Schema` setup, you can now move onto ingesting data.\n",
    "\n",
    "### Overview of the Dataset\n",
    "\n",
    "The dataset you will be using is an ensembled **Heart Disease Dataset**, which consists of a combination of the 5 popular heart disease datasets:\n",
    "\n",
    " - Cleveland\n",
    " - Hungarian\n",
    " - Switzerland\n",
    " - Long Beach VA\n",
    " - Statlog (Heart) Data Set\n",
    "\n",
    "The dataset consists of 1190 rows, with 11 features (attributes) and 1 target (column to predict):\n",
    "\n",
    "| Feature Name          | Description                                                                   |Data Type  | Type                  |\n",
    "|-----------------------|-------------------------------------------------------------------------------|-----------|-----------------------|\n",
    "| age                   | Age of the patient                                                            | int       | Numeric               |\n",
    "| sex                   | Gender of the patient (1 = male, 0 = female)                                  | int       | Categorical (binary)  |\n",
    "| chest pain type       | Type of chest pain (values: 1, 2, 3, 4)                                       | int       | Categorical (ordinal) |\n",
    "| resting bp s          | Resting blood pressure                                                        | int       | Numeric               |\n",
    "| cholesterol           | Serum cholesterol in mg/dl                                                    | int       | Numeric               |\n",
    "| fasting blood sugar   | Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)                         | int       | Categorical (binary)  |\n",
    "| resting ecg           | Resting electrocardiographic results (values: 0, 1, 2)                        | int       | Categorical (ordinal) |\n",
    "| max heart rate        | Maximum heart rate                                                            | int       | Numeric               |\n",
    "| exercise angina       | Exercise-induced angina (1 = yes, 0 = no)                                     | int       | Categorical (binary)  |\n",
    "| old peak              | Depression induced by exercise relative to rest (numeric value)\t            | float     | Numeric               |\n",
    "| ST slope              | Slope of the peak exercise ST segment (values: 1, 2, 3)\t                    | int       | Categorical (ordinal) |\n",
    "| target                | Presence or absence of heart disease (1 = disease present, 0 = no disease)    | int       | Categorical (binary)  |\n",
    "\n",
    "The data is stored in `.csv` format.\n",
    "\n",
    "### Importing Data from a CSV File\n",
    "\n",
    "First you will setup the path to the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loc = os.path.abspath('')\n",
    "data_loc = f\"{loc}/heart_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset filename is called: `heart_data.csv` and therefore you append that to the path and read the file into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df = spark.read.csv(f'{data_loc}', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can show the top two rows of the dataframe by running the `.show()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also show the schema of the dataframe by running the `.printSchema()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue you need to address is that the columns in the currently imported dataframe are all of type string, which is not what you want (this is because csv data does not include types, therefore everything is assumed to be a string). You need the data types to match those defined in the table above, so you will have to cast the columns to their appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert the columns to the appropriate data types\n",
    "df_transformed = df.withColumn(\"age\", col(\"age\").cast(\"int\")) \\\n",
    "                   .withColumn(\"sex\", col(\"sex\").cast(\"int\")) \\\n",
    "                   .withColumn(\"chest pain type\", col(\"chest pain type\").cast(\"int\")) \\\n",
    "                   .withColumn(\"resting bp s\", col(\"resting bp s\").cast(\"int\")) \\\n",
    "                   .withColumn(\"cholesterol\", col(\"cholesterol\").cast(\"int\")) \\\n",
    "                   .withColumn(\"fasting blood sugar\", col(\"fasting blood sugar\").cast(\"int\")) \\\n",
    "                   .withColumn(\"resting ecg\", col(\"resting ecg\").cast(\"int\")) \\\n",
    "                   .withColumn(\"max heart rate\", col(\"max heart rate\").cast(\"int\")) \\\n",
    "                   .withColumn(\"exercise angina\", col(\"exercise angina\").cast(\"int\")) \\\n",
    "                   .withColumn(\"oldpeak\", col(\"oldpeak\").cast(\"float\")) \\\n",
    "                   .withColumn(\"target\", col(\"target\").cast(\"int\")) \\\n",
    "                   .withColumn(\"ST slope\", col(\"ST slope\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can print out the new transformed dataframe, and you see that the types are now correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Table\n",
    "\n",
    "To create a table in an Iceberg-managed format, you can use the following SQL statement within Spark. This ensures that the table is created with the correct schema and is managed within your Iceberg catalog.\n",
    "\n",
    "The following code creates a `Table` called `heart_data` in the `demo` `Schema` of the iceberg_data `Catalog`, ensuring that it only gets created if it doesn't already exist.\n",
    "\n",
    "If the table was created successfully the output will be an empty dataframe, this is because the `CREATE` commands typically do not return any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg_data.demo.heart_data (\n",
    "    age INT,\n",
    "    sex INT,\n",
    "    `chest pain type` INT,\n",
    "    `resting bp s` INT,\n",
    "    cholesterol INT,\n",
    "    `fasting blood sugar` INT,\n",
    "    `resting ecg` INT,\n",
    "    `max heart rate` INT,\n",
    "    `exercise angina` INT,\n",
    "    oldpeak FLOAT,\n",
    "    `ST slope` INT,\n",
    "    target INT\n",
    ")\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the table was created successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW TABLES IN iceberg_data.demo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the data from the transformed dataframe into the created Iceberg table, you can use the `write` method in Spark. The following steps outline how to insert the transformed dataframe into the heart_data table created in the Iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformed.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"iceberg_data.demo.heart_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run a query on the saved Iceberg table within watsonx.data to see if the information was saved successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM iceberg_data.demo.heart_data\n",
    "LIMIT 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! You have successfully ingested data into watsonx.data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Data quality is crucial for the success of AI models, and a major part of machine learning involves cleaning the data. While the dataset contains only around 1,000 rows, in real-world applications, datasets can contain trillions of rows or tokens.\n",
    "\n",
    "Traditional data processing methods, such as using Hadoop, typically involve reading and writing data to disk. Each read and write operation takes significant computation time, and transformations are applied sequentially.\n",
    "\n",
    "For example, if you had only 100MB of disk space but your dataset was 1GB (1,000MB), Hadoop would work as follows: it would load 100MB of data into memory, perform transformations, then load the next 100MB, and continue this process until the entire dataset has been processed.\n",
    "\n",
    "\n",
    "### Understanding the Value of Spark\n",
    "\n",
    "Spark revolutionizes data processing by offering two key advantages:\n",
    "\n",
    "- **In-Memory Computation:** Unlike Hadoop, which relies on disk storage, Spark loads data directly into memory. This approach is 10x to 100x faster since accessing memory is much quicker than reading from disk. This is especially advantageous for iterative algorithms, such as machine learning model training, which require repeated access to the data.\n",
    "\n",
    "- **Parallelism:** To maximize efficiency, Spark is designed to process data in parallel across distributed systems (i.e., across many nodes in a cluster). By splitting the dataset into smaller partitions and performing operations on them simultaneously, Spark avoids the bottleneck of sequential processing. This parallel processing allows Spark to handle very large datasets that do not fit into a single machine's memory by distributing the workload across many machines.\n",
    "\n",
    "By leveraging these capabilities, Spark can process data much faster, saving both time and resources.\n",
    "\n",
    "### Viewing the Data\n",
    "\n",
    "First you run a query to select the data from the `iceberg_data` on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM iceberg_data.demo.heart_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of a model is heavily influenced by the data it is trained on. To ensure better performance, you need to carefully examine and clean the dataset.\n",
    "\n",
    "For instance, a dataset may contain inherent biases. If certain columns are overrepresented, the model could become biased toward those features. Additionally, missing values in the dataset can disrupt the training process and lead to inaccurate predictions or lower model performance.\n",
    "\n",
    "While there are many aspects to consider when cleaning a dataset, for the purposes of this lab, you will only focus on handling missing values.\n",
    "\n",
    "In this lab, you will walk through each column of the dataset and check for any missing (`null`) values, which you will then address appropriately to ensure a cleaner and more reliable dataset for model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of columns with null values\n",
    "null_columns = [c for c in df_server.columns if df.filter(col(c).isNull()).count() > 0]\n",
    "\n",
    "print(\"Columns with null values:\", null_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, you can see that the column with null values is the chest pain type column.\n",
    "\n",
    "Before you address this, there is another important point about Spark. Up until now, you have mainly been running queries to work with the data. However, Spark also comes with a rich set of built-in data processing methods that are not only simple to use but also highly efficient. These methods can often be more convenient and performant than SQL queries, especially when working with large datasets.\n",
    "\n",
    "For the task of handling the null values in the chest pain type column, this lab will demonstrate how to approach this in two ways: first using SQL query syntax, and second using Spark's built-in data processing methods. This will give you the flexibility to choose the most suitable approach based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data (SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will need to see how many `null` values this column contains:\n",
    "\n",
    "- If its a small number, then you can directly remove those rows\n",
    "- If its a large number, then you can remove the column all togehter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Samples:\")\n",
    "spark.sql(\"SELECT count(*) AS count FROM iceberg_data.demo.heart_data\").show()\n",
    "print(\"Null Samples:\")\n",
    "spark.sql(\"SELECT count(*) AS count FROM iceberg_data.demo.heart_data AS t WHERE t.`chest pain type` IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there are only 5 rows with `null` values, therefore you will go with the first option and remove those rows all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server_cleaned = spark.sql(\"\"\"\n",
    "                      SELECT *\n",
    "                      FROM iceberg_data.demo.heart_data AS t \n",
    "                      WHERE t.`chest pain type` IS NOT NULL\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is it! Now I will show the method using Sparks built in data processing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data (Spark Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps will be the same, first seeing how many rows there are and how many contain `null` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Samples:\")\n",
    "print(df_server.count())\n",
    "print(\"Null Samples:\")\n",
    "print(df_server.filter(df_server[\"`chest pain type`\"].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then dropping the null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server = df_server.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Spark's built-in methods result in much shorter and cleaner code. These methods leverage the df_server variable, which has already been queried and stored in memory. This makes the approach more efficient, as it is operating on the cached data in memory rather than querying the data on the server repeatedly.\n",
    "\n",
    "Additionally, Spark provides easy-to-use, built-in functions for common tasks like dropping null values, making the code simpler and more concise. This approach allows you to take full advantage of Spark’s capabilities while keeping the code efficient and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current Length: {df_server.count()}\")\n",
    "print(f\"Current Null Samples: {df_server.filter(df_server['`chest pain type`'].isNull()).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming and Encoding Features\n",
    "\n",
    "Our classification model will be trained using a single vector that represents the data. The dataset contains two types of data: numerical and categorical.\n",
    "\n",
    "Categorical data can be further divided into two types: binary and ordinal.\n",
    "\n",
    "- Binary features take values of either `0` or `1`.\n",
    "- Ordinal features, on the other hand, represent categories that are ordered (e.g., `0, 1, ..., x`). While these values are numeric, they should not be treated as continuous numbers because they represent distinct categories. Unlike numerical features, where you might identify relationships (e.g., identifying a threshold for resting heart rate indicating potential heart disease), ordinal features should not have relationships inferred between the numbers themselves.\n",
    "\n",
    "For example, consider the ordinal feature height with values `0`, `1`, and `2`, corresponding to `short`, `average`, and `tall`. It doesn't make sense to infer that the model should treat `0 (short)`, `1 (average)`, and `2 (tall)` as ordered numbers. Instead, you can represent this data using one-hot encoding, where each category is represented as a binary vector: `[1, 0, 0]` for short, `[0, 1, 0]` for average, and `[0, 0, 1]` for tall. This ensures that the model sees these categories as mutually exclusive, not as a numeric range.\n",
    "\n",
    "The next step is to convert these ordinal categorical features into one-hot encodings for better training.\n",
    "\n",
    "From the data representation table given above, you can identify the following features as ordinal categorical features:\n",
    "- Chest pain type\n",
    "- Resting ECG\n",
    "- ST slope\n",
    "\n",
    "Since the binary features do not require any transformation, you will include them in the list of numerical features. These binary features already take values of 0 or 1, so they don't need further encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = [\"chest pain type\", \"resting ecg\", \"ST slope\"]\n",
    "num_cols = [x for x in df_server.columns if (x not in cat_cols) & (x != \"target\")]\n",
    "\n",
    "print(f\"Categorical Features: {cat_cols}\")\n",
    "print(f\"Numerical Features: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at the distribution of these ordinal categorical features by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feat in cat_cols:\n",
    "    print(f\"Feature: {feat}\")\n",
    "    df_server.groupBy(feat).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the chest pain type feature contains values: `1`, `2`, `3`, and `4`. To represent this feature using one-hot encoding, you convert each value into a binary vector. For example, the value 4 will be represented as `[0, 0, 0, 1]`, where each index corresponds to one of the four possible categories.\n",
    "\n",
    "This way, the model will treat each category as mutually exclusive, avoiding any misleading relationships between the values.\n",
    "\n",
    "Next you will be using Spark’s `OneHotEncoder` to transform the categorical features into one-hot encoded vectors. The `OneHotEncoder` is a part of the `pyspark.ml.feature` module, which provides useful data transformation methods tailored for machine learning workflows.\n",
    "\n",
    "It is useful because it:\n",
    "- is optimized for distributed computing.\n",
    "- is a part of Spark’s MLlib (Machine Learning Library), which allows for seamless integration into machine learning pipelines.\n",
    "- is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = [\n",
    "    OneHotEncoder(inputCols=cat_cols, \n",
    "                  outputCols=[f\"{x}_OneHotEncoder\" for x in cat_cols])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `OneHotEncoder` will have the encoded ordinal categorical features with the suffix `_OneHotEncoder`. These newly created columns represent the one-hot encoded version of each categorical feature, turning them into binary vectors.\n",
    "\n",
    "Next, you will need to combine all the selected features — both the numerical columns and the one-hot encoded categorical columns — into a single vector column using Spark's `VectorAssembler`. This step is necessary because machine learning algorithms in Spark require the input features to be in a single vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemblerInput = [x for x in num_cols]\n",
    "assemblerInput += [f\"{x}_OneHotEncoder\" for x in cat_cols]\n",
    "\n",
    "print(f\"Assembler inputs are: \\n{assemblerInput}\")\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=assemblerInput, outputCol=\"VectorAssembler_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data for Training and Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you split the data into training data and testing data with a `80/20` split: `80%` training and `20%` testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_server.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "print(f\"Train data length: {train_df.count()}\")\n",
    "print(f\"Test data length:  {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Spark Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you wil setup a Spark pipeline to apply a sequence of data transformations.\n",
    "\n",
    "- `Pipeline`: A pipeline allows you to chain multiple stages of data processing into one object, making it easy to apply the same transformations to both training and test datasets.\n",
    "- `stages`: This list contains the transformation steps that will be applied in order. In this case, it includes the one-hot encoding of categorical features and the assembly of features into a single vector using VectorAssembler.\n",
    "- `pipeline`: The pipeline object is created and the stages are set. When applied, it will automatically execute the transformations in the correct order (first one-hot encoding, then vector assembly).\n",
    "  \n",
    "The benefit of using a pipeline is that it organizes the data processing steps into a streamlined, reusable workflow, ensuring consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = one_hot_encoder + [vector_assembler]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will fit the pipeline to the training data, learning the necessary transformations, and then apply the fitted pipeline to transform the test set, ensuring it undergoes the same preprocessing as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "fitted_pipeline = pipeline.fit(train_df)\n",
    "\n",
    "pp_train_df = fitted_pipeline.transform(train_df)\n",
    "pp_test_df = fitted_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will check if the transformed data, `pp_train_df`, is correct. \n",
    "\n",
    "Do not be alarmed by the output format. While you expected one vector with the desired values, some rows may appear as tuples because they are using a sparse vector format. This format helps save memory while still representing the same information.\n",
    "\n",
    "In the sparse vector format:\n",
    "\n",
    "- `Index 0`: Represents the length of the vector.\n",
    "- `Index 1`: Lists the indices of the non-zero values.\n",
    "- `Index 2`: Contains the actual non-zero values.\n",
    "This efficient representation allows Spark to handle large datasets more effectively by only storing non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_train_df.select(\n",
    "    'VectorAssembler_features'\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Preparing data for model training\n",
    "\n",
    "Before training the model, you need to ensure that the data is in the correct format. Specifically, you need to:\n",
    "\n",
    "- **Select the features:** Use the transformed features, which are represented by the VectorAssembler_features column.\n",
    "- **Rename the target column:** The column containing the labels should be named label (as required by many Spark MLlib models like `LogisticRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "train_data = pp_train_df.select(F.col(\"VectorAssembler_features\").alias(\"features\"), F.col(\"target\").alias(\"label\"))\n",
    "test_data = pp_test_df.select(F.col(\"VectorAssembler_features\").alias(\"features\"), F.col(\"target\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "You will be using Logistic Regression as the classification model. If you are interested in learning more about what Logistic Regression is, I highly recommend checking out this article by IBM: [Logistic Regression](https://www.ibm.com/think/topics/logistic-regression). However going into detail is out of scope of this lab.\n",
    "\n",
    "Steps:\n",
    "1. **Initialize the Model:** You start by initializing a Logistic Regression model using LogisticRegression() from PySpark's MLlib.\n",
    "2. **Fit the Model:** Next, you fit the model to the train_data dataset, where the model learns the relationship between the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(train_data)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the performance of the model on the train set by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model performed pretty well, with a 90+ percent accuracy on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model\n",
    "\n",
    "Now that the model is trained and shows good performance on the training data, the next critical step is to evaluate how well the model generalizes. This means testing whether the model performs well on unseen data (the test set), which gives you an indication of how it will perform in real-world scenarios when dealing with new data.\n",
    "\n",
    "To evaluate the performance of the model on the test data, you will use the `MulticlassClassificationEvaluator` from `PySpark`. This evaluator helps compute various metrics like accuracy, precision, recall, and F1 score. For this example, you will focus on accuracy.\n",
    "\n",
    "Steps to Evaluate the Model:\n",
    "1. **Get Predictions from the test_data:** Use the trained model to predict on the test data.\n",
    "2. **Evaluate Accuracy:** Use the MulticlassClassificationEvaluator to compute the accuracy by comparing the predicted labels with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Use MulticlassClassificationEvaluator to calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy on test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model performs well on unseen data as well! With that, the lab is concluded. I hope you enjoyed this lab and learned a lot about the advantages of Spark and Spark Lab.\n",
    "\n",
    "All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
