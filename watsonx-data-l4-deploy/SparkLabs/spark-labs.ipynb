{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Labs\n",
    "## Introduction \n",
    "Welcome to the Spark Labs Lab Guide! In this lab, we’ll walk through a hands-on scenario where you'll learn how to leverage Apache Spark to:\n",
    "\n",
    "- Load data efficiently\n",
    "- Save data into a table\n",
    "- Clean and preprocess data using Spark's parallel processing capabilities\n",
    "- Train a simple Logistic Regression classification model on the processed data\n",
    "\n",
    "### Spark Labs Benefits:\n",
    "Before we dive into the lab, let’s take a moment to discuss the benefits of using Spark Labs with watsonx.data, especially in comparison to using external notebooks that connects to watsonx.data.\n",
    "\n",
    "\n",
    "1.  No Dependency Management:\n",
    "    - External Notebooks: When working in an external notebook, you'd typically run your code with the dependencies installed on your own computer. This can lead to version mismatches, dependency conflicts, or other setup challenges.\n",
    "    - watsonx.data: Since Spark Labs runs directly within the watsonx.data environment, you don’t need to worry about managing or troubleshooting dependencies. All the necessary packages and configurations are preconfigured on the server, ensuring consistency and saving you time.\n",
    "  \n",
    "\n",
    "2.  Easier Configuration:\n",
    "    - External Notebooks: Setting up an external environment often requires manual configuration, from installing the right versions of Spark and libraries to setting up paths and environment variables.\n",
    "    - watsonx.data: With watsonx.data, the environment is preconfigured for you. You can jump straight into coding without spending time on setup, making the process much faster and smoother.\n",
    "  \n",
    "\n",
    "3.  Integrated Development Environment:\n",
    "    - External Notebooks: Using external notebooks can require managing different tools and platforms to run Spark, track results, and visualize data, which can be disjointed and time-consuming.\n",
    "    - watsonx.data: In this environment, everything you need is integrated into a single platform. You can manage your notebooks, run Spark jobs, and view results all in real time, without switching between different tools.\n",
    "\n",
    "\n",
    "4. Easy Collaboration:\n",
    "   - External Notebooks: Collaboration can be cumbersome when working in local environments. Sharing notebooks may involve version control, syncing, or exporting and sending files manually.\n",
    "   - watsonx.data: With notebooks stored directly on the server, collaboration is seamless. You can easily share your work with colleagues and stakeholders, ensuring that everyone is on the same page and able to provide feedback in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session Setup \n",
    "\n",
    "### Traditional Method (External Notebooks)\n",
    "\n",
    "Before we can start using Apache Spark, we need to set up a Spark Session. This session is crucial as it acts as the entry point for all Spark functionality, allowing us to interact with Spark’s powerful distributed computing capabilities.\n",
    "\n",
    "In external environments, the setup typically involves configuring several options like the application name, cluster manager, memory allocation, and more. For example:\n",
    "```python\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"ComplexSparkSessionSetup\") \\\n",
    "    .setMaster(\"yarn\") \\  # Using YARN as the cluster manager\n",
    "    .set(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\  # Custom warehouse location\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"500\") \\  # Increase shuffle partitions for performance\n",
    "    .set(\"spark.executor.memory\", \"8g\") \\  # Allocate 8GB of memory to each executor\n",
    "    .set(\"spark.driver.memory\", \"4g\") \\  # Allocate 4GB of memory to the driver\n",
    "    .set(\"spark.executor.cores\", \"4\") \\  # 4 cores per executor\n",
    "    .set(\"spark.sql.parquet.compression.codec\", \"snappy\") \\  # Use Snappy compression for Parquet files\n",
    "    .set(\"spark.ui.port\", \"4041\")  # Change the Spark UI port to avoid conflicts\n",
    "```\n",
    "\n",
    "However, when running within watsonx.data, this extensive configuration is unnecessary because it is automatically setup, streamlining the setup processes, saving time and avoiding the need to sift through documentation.\n",
    "\n",
    "### Simplified Setup for Watsonx.data (Spark Labs)\n",
    "\n",
    "In this current lab, the only configuration needed is the `fs.s3a.path.style.access` flag, which should be set to `true`. This is required because we're working with a `MinIO` bucket, and this flag ensures that Spark can properly access it.\n",
    "\n",
    "#### Additional Useful Flags:\n",
    "There are a few other configuration options that can help fine-tune your Spark session, especially for logging:\n",
    "\n",
    "- `ae.spark.driver.log.level`: Sets the log level for the driver (e.g. `\"INFO\"`)\n",
    "- `ae.spark.executor.log.level`: Sets the log level for the executro (e.g. `\"INFO\"`)\n",
    "\n",
    "These flags are optional but can be helpful for debugging or performance tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/10 14:38:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/10 14:38:27 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/03/10 14:38:35 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "spark = SparkSession.builder.appName('sparky').getOrCreate()\n",
    "conf=spark.sparkContext.getConf()\n",
    "spark.stop()\n",
    "\n",
    "conf.setAll([\n",
    "    (\"fs.s3a.path.style.access\", \"true\"),\n",
    "    # (\"ae.spark.driver.log.level\", \"INFO\"),\n",
    "    # (\"ae.spark.executor.log.level\", \"INFO\"),\n",
    "])\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that spark session and see the current version of spark, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Database and Creating Schemas\n",
    "\n",
    "Watsonx.data follows a cascading architecture, structured in a hierarchical form: `Catalog` > `Schema` > `Table`. This topology helps organize the data in a way that is intuitive and easy to navigate.\n",
    "\n",
    "- `Catalog`: The top level in the architecture. Each Catalog contains multiple Schemas. It acts as a container for related datasets.\n",
    "- `Schema`: The middle level. A Schema organizes multiple Tables within a catalog. It serves as a namespace for tables, often reflecting logical groupings of data.\n",
    "- `Table`: The bottom level, where actual data resides. Each Table follows a specific schema definition and contains the data, often in a structured format such as rows and columns.\n",
    "\n",
    "### Syntax for working with the hierachy:\n",
    "\n",
    "In Watsonx.data, the hierarchy of `Catalogs`, `Schemas`, and `Tables` is accessed using `dot notation`. This makes it easy to specify and query data at various levels of the hierarchy.\n",
    "\n",
    "For example, if you wanted to run queries on the Table `exampleTable` within the Schema `exampleSchema`, which is contained in the Catalog `exampleCatalog`, you would refer to it using the following syntax:\n",
    "```python\n",
    "exampleCatalog.exampleSchema.exampleTable\n",
    "```\n",
    "\n",
    "### Viewing our Catalogs\n",
    "To view the available `Catalogs` in Watsonx.data, you can use the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 14:38:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(scavenge), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "25/03/10 14:38:51 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(global, scavenge), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW CATALOGS\n",
    "\"\"\").show()\n",
    "\n",
    "# Note: the .show() method displays outputs in a human readable table format as seen below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing Schemas within a Catalog:\n",
    "To view the `Schemas` within a catalog, you can use the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 14:39:18 WARN HiveConf: HiveConf of name hive.metastore.truststore.type does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|wxd_system_data_d...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW SCHEMAS IN iceberg_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Schema\n",
    "Before we proceed with data ingestion, we need to create a new `Schema` where the newly created `Table` will reside.\n",
    "\n",
    "In this case, we will create a `Schema` called `demo` within the `Catalog` `iceberg_data`. Using dot notation, the full reference to this Schema will be `iceberg_data.demo`.\n",
    "\n",
    "To create the new `Schema`, run the following SQL command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE SCHEMA IF NOT EXISTS iceberg_data.demo\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll double check to see if the `Schema` was created correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|             default|\n",
      "|wxd_system_data_d...|\n",
      "|                demo|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW SCHEMAS IN iceberg_data\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is now a `demo` in the returned table, so the Schema creation was successful and we can move on to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Injestion\n",
    "\n",
    "Now that we have the needed `Catalog` and `Schema` setup, we can now move onto injesting data.\n",
    "\n",
    "### Dataset Overview\n",
    "\n",
    "The dataset we'll be using is an ensambled **Heart Disease Dataset**, which consists of a combination of the 5 popular heart disease datasets:\n",
    "\n",
    " - Cleveland\n",
    " - Hungarian\n",
    " - Switzerland\n",
    " - Long Beach VA\n",
    " - Statlog (Heart) Data Set.\n",
    "\n",
    "The dataset consists of 1190 rows, with 11 features and 1 target:\n",
    "\n",
    "| Feature Name          | Description                                                                   |Data Type  | Type                  |\n",
    "|-----------------------|-------------------------------------------------------------------------------|-----------|-----------------------|\n",
    "| age                   | Age of the patient                                                            | int       | Numeric               |\n",
    "| sex                   | Gender of the patient (1 = male, 0 = female)                                  | int       | Categorical (binary)  |\n",
    "| chest pain type       | Type of chest pain (values: 1, 2, 3, 4)                                       | int       | Categorical (ordinal) |\n",
    "| resting bp s          | Resting blood pressure                                                        | int       | Numeric               |\n",
    "| cholesterol           | Serum cholesterol in mg/dl                                                    | int       | Numeric               |\n",
    "| fasting blood sugar   | Fasting blood sugar > 120 mg/dl (1 = true, 0 = false)                         | int       | Categorical (binary)  |\n",
    "| resting ecg           | Resting electrocardiographic results (values: 0, 1, 2)                        | int       | Categorical (ordinal) |\n",
    "| max heart rate        | Maximum heart rate                                                            | int       | Numeric               |\n",
    "| exercise angina       | Exercise-induced angina (1 = yes, 0 = no)                                     | int       | Categorical (binary)  |\n",
    "| old peak              | Depression induced by exercise relative to rest (numeric value)\t            | float     | Numeric               |\n",
    "| ST slope              | Slope of the peak exercise ST segment (values: 1, 2, 3)\t                    | int       | Categorical (ordinal) |\n",
    "| target                | Presence or absence of heart disease (1 = disease present, 0 = no disease)    | int       | Categorical (binary)  |\n",
    "\n",
    "The data is stored in `.csv` format.\n",
    "\n",
    "### Importing Data from a CSV File\n",
    "\n",
    "First we'll setup the path to our file, in this example we'll be storing the data in the `/data` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "loc = os.path.abspath('')\n",
    "data_loc = f\"{loc}/heart_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset filename is called: `heart_data.csv` therefore we append that to the path and read the file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 14:40:35 WARN SizeEstimator: Failed to check whether UseCompressedOops is set; assuming yes\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "df = spark.read.csv(f'{data_loc}', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can show the top two rows of the dataframe by running the `.show()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "|age|sex|chest pain type|resting bp s|cholesterol|fasting blood sugar|resting ecg|max heart rate|exercise angina|oldpeak|ST slope|target|\n",
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "| 40|  1|              2|         140|        289|                  0|          0|           172|              0|    0.0|       1|     0|\n",
      "| 49|  0|              3|         160|        180|                  0|          0|           156|              0|    1.0|       2|     1|\n",
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also show the schema of the dataframe by running the `.printSchema()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- chest pain type: string (nullable = true)\n",
      " |-- resting bp s: string (nullable = true)\n",
      " |-- cholesterol: string (nullable = true)\n",
      " |-- fasting blood sugar: string (nullable = true)\n",
      " |-- resting ecg: string (nullable = true)\n",
      " |-- max heart rate: string (nullable = true)\n",
      " |-- exercise angina: string (nullable = true)\n",
      " |-- oldpeak: string (nullable = true)\n",
      " |-- ST slope: string (nullable = true)\n",
      " |-- target: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue we need to address is that the columns in the currently imported DataFrame are all of type string, which isn't what we want (this is because csv data doesn't include types, therefore everything is assumed to be a string). We need the data types to match those defined in the table above, so we'll have to cast the columns to their appropriate types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Convert the columns to the appropriate data types\n",
    "df_transformed = df.withColumn(\"age\", col(\"age\").cast(\"int\")) \\\n",
    "                   .withColumn(\"sex\", col(\"sex\").cast(\"int\")) \\\n",
    "                   .withColumn(\"chest pain type\", col(\"chest pain type\").cast(\"int\")) \\\n",
    "                   .withColumn(\"resting bp s\", col(\"resting bp s\").cast(\"int\")) \\\n",
    "                   .withColumn(\"cholesterol\", col(\"cholesterol\").cast(\"int\")) \\\n",
    "                   .withColumn(\"fasting blood sugar\", col(\"fasting blood sugar\").cast(\"int\")) \\\n",
    "                   .withColumn(\"resting ecg\", col(\"resting ecg\").cast(\"int\")) \\\n",
    "                   .withColumn(\"max heart rate\", col(\"max heart rate\").cast(\"int\")) \\\n",
    "                   .withColumn(\"exercise angina\", col(\"exercise angina\").cast(\"int\")) \\\n",
    "                   .withColumn(\"oldpeak\", col(\"oldpeak\").cast(\"float\")) \\\n",
    "                   .withColumn(\"target\", col(\"target\").cast(\"int\")) \\\n",
    "                   .withColumn(\"ST slope\", col(\"ST slope\").cast(\"int\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can print out the new transformed DataFrame, and we see that the types are now correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: integer (nullable = true)\n",
      " |-- sex: integer (nullable = true)\n",
      " |-- chest pain type: integer (nullable = true)\n",
      " |-- resting bp s: integer (nullable = true)\n",
      " |-- cholesterol: integer (nullable = true)\n",
      " |-- fasting blood sugar: integer (nullable = true)\n",
      " |-- resting ecg: integer (nullable = true)\n",
      " |-- max heart rate: integer (nullable = true)\n",
      " |-- exercise angina: integer (nullable = true)\n",
      " |-- oldpeak: float (nullable = true)\n",
      " |-- ST slope: integer (nullable = true)\n",
      " |-- target: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transformed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Table\n",
    "\n",
    "To create a table in an Iceberg-managed format, you can use the following SQL statement within Spark. This ensures that the table is created with the correct schema and is managed within your Iceberg catalog.\n",
    "\n",
    "The following code creates a `Table` called `heart_data` in the `demo` `Schema` of the iceberg_data `Catalog`, ensuring that it only gets created if it doesn't already exist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/10 14:41:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/03/10 14:41:20 WARN VersionInfoUtils: The AWS SDK for Java 1.x entered maintenance mode starting July 31, 2024 and will reach end of support on December 31, 2025. For more information, see https://aws.amazon.com/blogs/developer/the-aws-sdk-for-java-1-x-is-in-maintenance-mode-effective-july-31-2024/\n",
      "You can print where on the file system the AWS SDK for Java 1.x core runtime is located by setting the AWS_JAVA_V1_PRINT_LOCATION environment variable or aws.java.v1.printLocation system property to 'true'.\n",
      "This message can be disabled by setting the AWS_JAVA_V1_DISABLE_DEPRECATION_ANNOUNCEMENT environment variable or aws.java.v1.disableDeprecationAnnouncement system property to 'true'.\n",
      "The AWS SDK for Java 1.x is being used here:\n",
      "at java.base/java.lang.Thread.getStackTrace(Thread.java:1188)\n",
      "at com.amazonaws.util.VersionInfoUtils.printDeprecationAnnouncement(VersionInfoUtils.java:81)\n",
      "at com.amazonaws.util.VersionInfoUtils.<clinit>(VersionInfoUtils.java:59)\n",
      "at com.amazonaws.ClientConfiguration.<clinit>(ClientConfiguration.java:95)\n",
      "at org.apache.hadoop.fs.s3a.S3AUtils.createAwsConf(S3AUtils.java:1258)\n",
      "at org.apache.hadoop.fs.s3a.DefaultS3ClientFactory.createS3Client(DefaultS3ClientFactory.java:114)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.bindAWSClient(S3AFileSystem.java:898)\n",
      "at org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:534)\n",
      "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3611)\n",
      "at org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3712)\n",
      "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3663)\n",
      "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n",
      "at org.apache.iceberg.hadoop.Util.getFs(Util.java:56)\n",
      "at org.apache.iceberg.hadoop.HadoopOutputFile.fromPath(HadoopOutputFile.java:53)\n",
      "at org.apache.iceberg.hadoop.HadoopFileIO.newOutputFile(HadoopFileIO.java:97)\n",
      "at org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadata(BaseMetastoreTableOperations.java:158)\n",
      "at org.apache.iceberg.BaseMetastoreTableOperations.writeNewMetadataIfRequired(BaseMetastoreTableOperations.java:153)\n",
      "at org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:174)\n",
      "at org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:128)\n",
      "at org.apache.iceberg.BaseMetastoreCatalog$BaseMetastoreCatalogTableBuilder.create(BaseMetastoreCatalog.java:201)\n",
      "at org.apache.iceberg.CachingCatalog$CachingTableBuilder.lambda$create$0(CachingCatalog.java:262)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)\n",
      "at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1916)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)\n",
      "at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)\n",
      "at org.apache.iceberg.CachingCatalog$CachingTableBuilder.create(CachingCatalog.java:258)\n",
      "at org.apache.iceberg.spark.SparkCatalog.createTable(SparkCatalog.java:247)\n",
      "at org.apache.spark.sql.connector.catalog.TableCatalog.createTable(TableCatalog.java:199)\n",
      "at org.apache.spark.sql.execution.datasources.v2.CreateTableExec.run(CreateTableExec.scala:44)\n",
      "at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "at org.apache.spark.sql.Dataset.<init>(Dataset.scala:218)\n",
      "at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:95)\n",
      "at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:640)\n",
      "at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:630)\n",
      "at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:662)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "at java.base/java.lang.reflect.Method.invoke(Method.java:572)\n",
      "at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "at py4j.Gateway.invoke(Gateway.java:282)\n",
      "at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "at py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "at java.base/java.lang.Thread.run(Thread.java:839)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS iceberg_data.demo.heart_data (\n",
    "    age INT,\n",
    "    sex INT,\n",
    "    `chest pain type` INT,\n",
    "    `resting bp s` INT,\n",
    "    cholesterol INT,\n",
    "    `fasting blood sugar` INT,\n",
    "    `resting ecg` INT,\n",
    "    `max heart rate` INT,\n",
    "    `exercise angina` INT,\n",
    "    oldpeak FLOAT,\n",
    "    `ST slope` INT,\n",
    "    target INT\n",
    ")\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the table was created successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----------+\n",
      "|namespace| tableName|isTemporary|\n",
      "+---------+----------+-----------+\n",
      "|     demo|heart_data|      false|\n",
      "+---------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SHOW TABLES IN iceberg_data.demo\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To write the data from the transformed DataFrame into the created Iceberg table, you can use the `write` method in Spark. The following steps outline how to insert the transformed DataFrame into the heart_data table created in the Iceberg catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_transformed.write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save(\"iceberg_data.demo.heart_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets run a query on the saved Iceberg table within watsonx.data to see if the information was saved successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "|age|sex|chest pain type|resting bp s|cholesterol|fasting blood sugar|resting ecg|max heart rate|exercise angina|oldpeak|ST slope|target|\n",
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "| 40|  1|              2|         140|        289|                  0|          0|           172|              0|    0.0|       1|     0|\n",
      "| 49|  0|              3|         160|        180|                  0|          0|           156|              0|    1.0|       2|     1|\n",
      "+---+---+---------------+------------+-----------+-------------------+-----------+--------------+---------------+-------+--------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM iceberg_data.demo.heart_data\n",
    "LIMIT 2\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voilà! We have successfully ingested data into watsonx.data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Data quality is crucial for the success of AI models, and a major part of machine learning involves cleaning the data. While our dataset contains only around 1,000 rows, in real-world applications, datasets can contain trillions of rows or tokens.\n",
    "\n",
    "Traditional data processing methods, such as using Hadoop, typically involve reading and writing data to disk. Each read and write operation takes significant computation time, and transformations are applied sequentially.\n",
    "\n",
    "For example, if you had only 100MB of disk space but your dataset was 1GB (1,000MB), Hadoop would work as follows: it would load 100MB of data into memory, perform transformations, then load the next 100MB, and continue this process until the entire dataset has been processed.\n",
    "\n",
    "\n",
    "### The Value of Spark\n",
    "\n",
    "Spark revolutionizes data processing by offering two key advantages:\n",
    "\n",
    "- **In-Memory Computation:** Unlike Hadoop, which relies on disk storage, Spark loads data directly into memory. This approach is 10x to 100x faster since accessing memory is much quicker than reading from disk. This is especially advantageous for iterative algorithms, such as machine learning model training, which require repeated access to the data.\n",
    "\n",
    "- **Parallelism:** To maximize efficiency, Spark is designed to process data in parallel across distributed systems (i.e., across many nodes in a cluster). By splitting the dataset into smaller partitions and performing operations on them simultaneously, Spark avoids the bottleneck of sequential processing. This parallel processing allows Spark to handle very large datasets that don't fit into a single machine's memory by distributing the workload across many machines.\n",
    "\n",
    "By leveraging these capabilities, Spark can process data much faster, saving both time and resources.\n",
    "\n",
    "### Viewing the Data\n",
    "\n",
    "First we run a query to select the data from our `iceberg_data` on the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server = spark.sql(\"\"\"\n",
    "SELECT *\n",
    "FROM iceberg_data.demo.heart_data\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of a model is heavily influenced by the data it's trained on. To ensure better performance, we need to carefully examine and clean the dataset.\n",
    "\n",
    "For instance, a dataset may contain inherent biases. If certain columns are overrepresented, the model could become biased toward those features. Additionally, missing values in the dataset can disrupt the training process and lead to inaccurate predictions or lower model performance.\n",
    "\n",
    "While there are many aspects to consider when cleaning a dataset, for the purposes of this lab, we will only focus on handling missing values.\n",
    "\n",
    "In this lab, we'll walk through each column of the dataset and check for any missing (`null`) values, which we will then address appropriately to ensure a cleaner and more reliable dataset for model training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with null values: ['chest pain type']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# List of columns with null values\n",
    "null_columns = [c for c in df_server.columns if df.filter(col(c).isNull()).count() > 0]\n",
    "\n",
    "print(\"Columns with null values:\", null_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can see that the column with null values is the chest pain type column.\n",
    "\n",
    "Before we address this, there's another important point about Spark. Up until now, we've mainly been running queries to work with the data. However, Spark also comes with a rich set of built-in data processing methods that are not only simple to use but also highly efficient. These methods can often be more convenient and performant than SQL queries, especially when working with large datasets.\n",
    "\n",
    "For the task of handling the null values in the chest pain type column, I'll demonstrate how to approach this in two ways: first using SQL query syntax, and second using Spark's built-in data processing methods. This will give us the flexibility to choose the most suitable approach based on the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data (SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need to see how many `null` values this column contains:\n",
    "\n",
    "- If its a small number, then we can directly remove those rows\n",
    "- If its a large number, then we can remove the column all togehter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples:\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "| 1190|\n",
      "+-----+\n",
      "\n",
      "Null Samples:\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|    5|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Samples:\")\n",
    "spark.sql(\"SELECT count(*) AS count FROM iceberg_data.demo.heart_data\").show()\n",
    "print(\"Null Samples:\")\n",
    "spark.sql(\"SELECT count(*) AS count FROM iceberg_data.demo.heart_data AS t WHERE t.`chest pain type` IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are only 5 rows with `null` values, therefore we'll go with the first option and remove those rows all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server_cleaned = spark.sql(\"\"\"\n",
    "                      SELECT *\n",
    "                      FROM iceberg_data.demo.heart_data AS t \n",
    "                      WHERE t.`chest pain type` IS NOT NULL\n",
    "                      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! Now I will show the method using Sparks built in data processing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Data (Spark Methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps will be the same, first seeing how many rows there are and how mnay contain `null` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Samples:\n",
      "1190\n",
      "Null Samples:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Samples:\")\n",
    "print(df_server.count())\n",
    "print(\"Null Samples:\")\n",
    "print(df_server.filter(df_server[\"`chest pain type`\"].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then dropping the null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_server = df_server.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, Spark's built-in methods result in much shorter and cleaner code. These methods leverage the df_server variable, which has already been queried and stored in memory. This makes the approach more efficient, as it's operating on the cached data in memory rather than querying the data on the server repeatedly.\n",
    "\n",
    "Additionally, Spark provides easy-to-use, built-in functions for common tasks like dropping null values, making the code simpler and more concise. This approach allows us to take full advantage of Spark’s capabilities while keeping the code efficient and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Length: 1185\n"
     ]
    }
   ],
   "source": [
    "print(f\"Current Length: {df_server.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Transformation and Encoding\n",
    "\n",
    "Our classification model will be trained using a single vector that represents the data. The dataset contains two types of data: numerical and categorical.\n",
    "\n",
    "Categorical data can be further divided into two types: binary and ordinal.\n",
    "\n",
    "- Binary features take values of either `0` or `1`.\n",
    "- Ordinal features, on the other hand, represent categories that are ordered (e.g., `0, 1, ..., x`). While these values are numeric, they should not be treated as continuous numbers because they represent distinct categories. Unlike numerical features, where we might identify relationships (e.g., identifying a threshold for resting heart rate indicating potential heart disease), ordinal features should not have relationships inferred between the numbers themselves.\n",
    "\n",
    "For example, consider the ordinal feature height with values `0`, `1`, and `2`, corresponding to `short`, `average`, and `tall`. It doesn't make sense to infer that the model should treat `0 (short)`, `1 (average)`, and `2 (tall)` as ordered numbers. Instead, we can represent this data using one-hot encoding, where each category is represented as a binary vector: `[1, 0, 0]` for short, `[0, 1, 0]` for average, and `[0, 0, 1]` for tall. This ensures that the model sees these categories as mutually exclusive, not as a numeric range.\n",
    "\n",
    "The next step is to convert these ordinal categorical features into one-hot encodings for better training.\n",
    "\n",
    "From the data representation table given above, we can identify the following features as ordinal categorical features:\n",
    "- Chest pain type\n",
    "- Resting ECG\n",
    "- ST slope\n",
    "\n",
    "Since the binary features do not require any transformation, we will include them in the list of numerical features. These binary features already take values of 0 or 1, so they don't need further encoding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Features: ['chest pain type', 'resting ecg', 'ST slope']\n",
      "Numerical Features: ['age', 'sex', 'resting bp s', 'cholesterol', 'fasting blood sugar', 'max heart rate', 'exercise angina', 'oldpeak']\n"
     ]
    }
   ],
   "source": [
    "cat_cols = [\"chest pain type\", \"resting ecg\", \"ST slope\"]\n",
    "num_cols = [x for x in df_server.columns if (x not in cat_cols) & (x != \"target\")]\n",
    "\n",
    "print(f\"Categorical Features: {cat_cols}\")\n",
    "print(f\"Numerical Features: {num_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the distribution of these ordinal categorical features by doing the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: chest pain type\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|chest pain type|count|\n",
      "+---------------+-----+\n",
      "|              2|  215|\n",
      "|              3|  281|\n",
      "|              4|  624|\n",
      "|              1|   65|\n",
      "+---------------+-----+\n",
      "\n",
      "Feature: resting ecg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|resting ecg|count|\n",
      "+-----------+-----+\n",
      "|          0|  679|\n",
      "|          1|  181|\n",
      "|          2|  325|\n",
      "+-----------+-----+\n",
      "\n",
      "Feature: ST slope\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 56:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|ST slope|count|\n",
      "+--------+-----+\n",
      "|       1|  522|\n",
      "|       2|  581|\n",
      "|       3|   81|\n",
      "|       0|    1|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for feat in cat_cols:\n",
    "    print(f\"Feature: {feat}\")\n",
    "    df_server.groupBy(feat).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the chest pain type feature contains values: `1`, `2`, `3`, and `4`. To represent this feature using one-hot encoding, we convert each value into a binary vector. For example, the value 4 will be represented as `[0, 0, 0, 1]`, where each index corresponds to one of the four possible categories.\n",
    "\n",
    "This way, the model will treat each category as mutually exclusive, avoiding any misleading relationships between the values.\n",
    "\n",
    "Next we will be using Spark’s `OneHotEncoder` to transform the categorical features into one-hot encoded vectors. The `OneHotEncoder` is a part of the `pyspark.ml.feature` module, which provides useful data transformation methods tailored for machine learning workflows.\n",
    "\n",
    "It's useful because it:\n",
    "- is optimized for distributed computing.\n",
    "- is a part of Spark’s MLlib (Machine Learning Library), which allows for seamless integration into machine learning pipelines.\n",
    "- is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "one_hot_encoder = [\n",
    "    OneHotEncoder(inputCols=cat_cols, \n",
    "                  outputCols=[f\"{x}_OneHotEncoder\" for x in cat_cols])\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `OneHotEncoder` will have the encoded ordinal categorical features with the suffix `_OneHotEncoder`. These newly created columns represent the one-hot encoded version of each categorical feature, turning them into binary vectors.\n",
    "\n",
    "Next, we'll need to combine all the selected features — both the numerical columns and the one-hot encoded categorical columns — into a single vector column using Spark's `VectorAssembler`. This step is necessary because machine learning algorithms in Spark require the input features to be in a single vector format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembler inputs are: \n",
      "['age', 'sex', 'resting bp s', 'cholesterol', 'fasting blood sugar', 'max heart rate', 'exercise angina', 'oldpeak', 'chest pain type_OneHotEncoder', 'resting ecg_OneHotEncoder', 'ST slope_OneHotEncoder']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemblerInput = [x for x in num_cols]\n",
    "assemblerInput += [f\"{x}_OneHotEncoder\" for x in cat_cols]\n",
    "\n",
    "print(f\"Assembler inputs are: \\n{assemblerInput}\")\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=assemblerInput, outputCol=\"VectorAssembler_features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data to train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we split the data into training data and testing data with a `80/20` split: `80%` training and `20%` testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length: 946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data length:  239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "train_df, test_df = df_server.randomSplit([0.8, 0.2], seed=1234)\n",
    "\n",
    "print(f\"Train data length: {train_df.count()}\")\n",
    "print(f\"Test data length:  {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we wil setup a Spark pipeline to apply a sequence of data transformations.\n",
    "\n",
    "- `Pipeline`: A pipeline allows you to chain multiple stages of data processing into one object, making it easy to apply the same transformations to both training and test datasets.\n",
    "- `stages`: This list contains the transformation steps that will be applied in order. In this case, it includes the one-hot encoding of categorical features and the assembly of features into a single vector using VectorAssembler.\n",
    "- `pipeline`: The pipeline object is created and the stages are set. When applied, it will automatically execute the transformations in the correct order (first one-hot encoding, then vector assembly).\n",
    "  \n",
    "The benefit of using a pipeline is that it organizes the data processing steps into a streamlined, reusable workflow, ensuring consistency across datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages = one_hot_encoder + [vector_assembler]\n",
    "\n",
    "pipeline = Pipeline().setStages(stages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll fit the pipeline to the training data, learning the necessary transformations, and then apply the fitted pipeline to transform the test set, ensuring it undergoes the same preprocessing as the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 40.4 ms, sys: 11.6 ms, total: 52.1 ms\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fitted_pipeline = pipeline.fit(train_df)\n",
    "\n",
    "pp_train_df = fitted_pipeline.transform(train_df)\n",
    "pp_test_df = fitted_pipeline.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's check if our transformed data, `pp_train_df`, is correct. \n",
    "\n",
    "Don't be alarmed by the output format. While we expected one vector with the desired values, some rows may appear as tuples because they are using a sparse vector format. This format helps save memory while still representing the same information.\n",
    "\n",
    "In the sparse vector format:\n",
    "\n",
    "- `Index 0`: Represents the length of the vector.\n",
    "- `Index 1`: Lists the indices of the non-zero values.\n",
    "- `Index 2`: Contains the actual non-zero values.\n",
    "This efficient representation allows Spark to handle large datasets more effectively by only storing non-zero values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 60:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------+\n",
      "|VectorAssembler_features                                                          |\n",
      "+----------------------------------------------------------------------------------+\n",
      "|(17,[0,1,2,3,5,10,15],[28.0,1.0,130.0,132.0,185.0,1.0,1.0])                       |\n",
      "|(17,[0,1,2,3,5,10,15],[29.0,1.0,130.0,204.0,202.0,1.0,1.0])                       |\n",
      "|(17,[0,1,2,3,5,10,15],[29.0,1.0,130.0,204.0,202.0,1.0,1.0])                       |\n",
      "|(17,[0,2,3,5,9,13,15],[30.0,170.0,237.0,170.0,1.0,1.0,1.0])                       |\n",
      "|(17,[0,2,3,5,10,13,15],[31.0,100.0,219.0,150.0,1.0,1.0,1.0])                      |\n",
      "|(17,[0,1,2,3,5,6,7,12,16],[31.0,1.0,120.0,270.0,153.0,1.0,1.5,1.0,1.0])           |\n",
      "|(17,[0,2,3,5,10,12,15],[32.0,105.0,198.0,165.0,1.0,1.0,1.0])                      |\n",
      "|(17,[0,1,2,4,5,7,9,12,15],[32.0,1.0,95.0,1.0,127.0,0.699999988079071,1.0,1.0,1.0])|\n",
      "|(17,[0,1,2,3,5,10,12,15],[32.0,1.0,110.0,225.0,184.0,1.0,1.0,1.0])                |\n",
      "|(17,[0,1,2,3,5,10,12,15],[32.0,1.0,125.0,254.0,155.0,1.0,1.0,1.0])                |\n",
      "|(17,[0,1,2,3,5,12,16],[32.0,1.0,118.0,529.0,130.0,1.0,1.0])                       |\n",
      "|(17,[0,2,3,5,6,7,12,16],[33.0,100.0,246.0,150.0,1.0,1.0,1.0,1.0])                 |\n",
      "|(17,[0,2,3,5,7,10,12,15],[34.0,118.0,210.0,192.0,0.699999988079071,1.0,1.0,1.0])  |\n",
      "|(17,[0,2,3,5,7,10,12,15],[34.0,118.0,210.0,192.0,0.699999988079071,1.0,1.0,1.0])  |\n",
      "|(17,[0,2,3,5,10,12,15],[34.0,130.0,161.0,190.0,1.0,1.0,1.0])                      |\n",
      "|(17,[0,1,2,3,5,9,15],[34.0,1.0,118.0,182.0,174.0,1.0,1.0])                        |\n",
      "|(17,[0,1,2,3,5,9,15],[34.0,1.0,118.0,182.0,174.0,1.0,1.0])                        |\n",
      "|(17,[0,1,2,3,5,9,12,16],[34.0,1.0,140.0,156.0,180.0,1.0,1.0,1.0])                 |\n",
      "|(17,[0,1,2,3,5,10,12,15],[34.0,1.0,98.0,220.0,150.0,1.0,1.0,1.0])                 |\n",
      "|(17,[0,1,2,4,5,7,12,15],[34.0,1.0,115.0,1.0,154.0,0.20000000298023224,1.0,1.0])   |\n",
      "+----------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pp_train_df.select(\n",
    "    'VectorAssembler_features'\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### Preparing data for model training\n",
    "\n",
    "Before training the model, we need to ensure that the data is in the correct format. Specifically, we need to:\n",
    "\n",
    "- **Select the features:** Use the transformed features, which are represented by the VectorAssembler_features column.\n",
    "- **Rename the target column:** The column containing the labels should be named label (as required by many Spark MLlib models like `LogisticRegression`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "train_data = pp_train_df.select(F.col(\"VectorAssembler_features\").alias(\"features\"), F.col(\"target\").alias(\"label\"))\n",
    "test_data = pp_test_df.select(F.col(\"VectorAssembler_features\").alias(\"features\"), F.col(\"target\").alias(\"label\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "We'll be using Logistic Regression as the classification model. If you're interested in learning more about what Logistic Regression is, I highly recommend checking out this article by IBM: [Logistic Regression](https://www.ibm.com/think/topics/logistic-regression). However going into detail is out of scope of this lab.\n",
    "\n",
    "Steps:\n",
    "1. **Initialize the Model:** We start by initializing a Logistic Regression model using LogisticRegression() from PySpark's MLlib.\n",
    "2. **Fit the Model:** Next, we fit the model to the train_data dataset, where the model learns the relationship between the features and the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionModel: uid=LogisticRegression_1a70d05f5db6, numClasses=2, numFeatures=17\n",
      "CPU times: user 36.8 ms, sys: 13.5 ms, total: 50.3 ms\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "model = LogisticRegression().fit(train_data)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the performance of the model on the train set by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9250667923935244"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.summary.areaUnderROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the model performed pretty well!! With a 90+ percent accuracy on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Now that the model is trained and shows good performance on the training data, the next critical step is to evaluate how well the model generalizes. This means testing whether the model performs well on unseen data (the test set), which gives us an indication of how it will perform in real-world scenarios when dealing with new data.\n",
    "\n",
    "To evaluate the performance of the model on the test data, we'll use the `MulticlassClassificationEvaluator` from `PySpark`. This evaluator helps compute various metrics like accuracy, precision, recall, and F1 score. For this example, we'll focus on accuracy.\n",
    "\n",
    "Steps to Evaluate the Model:\n",
    "1. **Get Predictions from the test_data:** Use the trained model to predict on the test data.\n",
    "2. **Evaluate Accuracy:** Use the MulticlassClassificationEvaluator to compute the accuracy by comparing the predicted labels with the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 110:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.8285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Use MulticlassClassificationEvaluator to calculate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy on test data: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the model performs well on unseen data as well! With that, the lab is concluded. I hope you enjoyed this lab and learned a lot about the advantages of Spark and Spark Lab.\n",
    "\n",
    "All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
