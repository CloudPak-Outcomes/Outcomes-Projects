{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# 1. Insert project token, API key, and region\n\n<img src=\"https://cp4d-outcomes.techzone.ibm.com/img/data-fabric-lab/trusted-ai/project_token_for_notebook.png\" width=400 align=left>"}, {"metadata": {}, "cell_type": "markdown", "source": "Click the **three vertical dots** icon above and select **Insert project token** to provide this notebook API access to your project.\n\nThe code inserted above will have a line that looks like this:\n\n`project = Project(spark.sparkContext, 'xxxxxxxx-xxx-xxxx-xxxx-xxxxxxxxxx', 'p-xxxxxxxxxxxxxxxxxx')`\n\nThe first variable value from the cell above (the one that does **not** begin with `p-`) is your project ID, and should be pasted into the cell below as the value for `PROJECT_ID`. The API key you created earlier in the lab should be pasted into the cell below as the value for `API_KEY`.\n\nThe project ID value is also available on the **Manage** tab of your project, in the **General** section.\n\nThe **LOCATION** value below will depend on where you provisioned your services. According to the [WML Client documentation](https://ibm-wml-api-pyclient.mybluemix.net/#authentication), valid values for **LOCATION** are:\n* Dallas: https://us-south.ml.cloud.ibm.com\n* London: https://eu-gb.ml.cloud.ibm.com\n* Frankfurt: https://eu-de.ml.cloud.ibm.com\n* Tokyo: https://jp-tok.ml.cloud.ibm.com\n\nRun the cell above, and continue running cells individually until you reach step 2."}, {"metadata": {}, "cell_type": "code", "source": "API_KEY = 'xxxxxxxxxxxxxxxxxxx'\nPROJECT_ID = 'xxxxxxxxxxxxxxxxxx'\nLOCATION = 'https://us-south.ml.cloud.ibm.com'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "if \"p-\" in PROJECT_ID:\n    raise Exception(\"You have not correctly set the value for your PROJECT_ID. The value beginning with 'p-' is your project access \\\n    token. Please copy the value of the project_id into the previous cell and re-run it.\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The first model you will create in this notebook uses the scikit-learn framework. The `sklearn` package is available by default in Watson Studio Python environments, and does not need to be installed."}, {"metadata": {}, "cell_type": "code", "source": "import sklearn\nsklearn.__version__", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next cell uses the API key and location variables defined above to authenticate with your Watson Machine Learning service. An error in this cell likely means that you do not have access to a WML service, or that the API key or location provided above is incorrect."}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nwml_credentials = {\n    \"apikey\": API_KEY,\n    \"url\": LOCATION\n}\n\nwml_client = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:red\">2. !!--STOP--!! Insert data to code below</span>\n\nPlace your cursor in the empty code cell below. Then click the **Find and add data** icon in the upper right corner of the screen -- it looks like a grid of ones and zeroes.\n\n<img src=\"https://cp4d-outcomes.techzone.ibm.com/img/data-fabric-lab/trusted-ai/find_and_add_data.png\" width=400 align=left>"}, {"metadata": {}, "cell_type": "markdown", "source": "From the **Files** tab in the window that opens, locate the *modeling_records_2022.csv* file from the list of files beneath the drag and drop area, and use the **Insert to code** dropdown beneath it to select **pandas DataFrame**. A code block is automatically inserted into the empty cell that will import your data into a dataframe. Like the `sklearn` package, `pandas` is automatically provided in Watson Studio Python environments.\n\n## <span style=\"color:red\">IMPORTANT: replace all instances of `df_data_x` with `df_data_1` in the code</span>\n\nThe automated dataframe will likely use the `df_data_3` variable to hold the data. Update the last two lines of code to import data into the `df_data_1` variable for the rest of the notebook to work correctly. The last lines of your cell should look like this:\n\n<img src=\"https://cp4d-outcomes.techzone.ibm.com/img/data-fabric-lab/trusted-ai/dataframe_insert.png\" width=300 align=left>"}, {"metadata": {}, "cell_type": "markdown", "source": "Run the inserted code cell below. If you have correctly imported the data, you will see a table populated with employee data. Continue running cells individually until you reach step 3."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next cell splits the training data into the feature columns and the label columns, and then further splits the data further into a training data set and a testing data set. If this cell generates an error, it is likely because you have not imported the data into the `df_data_1` variable as described above. You will need to alter the previous cell to use `df_data_1` and then rerun it."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.model_selection import train_test_split\n\nX = df_data_1.drop(['ATTRITION'], axis=1)  # Features\ny = df_data_1['ATTRITION']  # Labels\n\n# Split dataset into training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15) # 85% training and 15% test", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now you will tell Watson Machine Learning to use the current project to store the model."}, {"metadata": {}, "cell_type": "code", "source": "X.columns.tolist()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The cell below tells the Watson Machine Learning client to save the models in the current project. If you receive an error here, it is likely because you did not correctly set your project ID at the beginning of the notebook."}, {"metadata": {}, "cell_type": "code", "source": "wml_client.set.default_project(PROJECT_ID)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The following cell provides connection information to the model training data, which will be stored with the model and in FactSheets. You could use the Cloud Object Storage information for this particular project by changing the credentials to match those from above where you inserted the file to code, but for simplicity's sake, you will use a pre-existing file."}, {"metadata": {}, "cell_type": "code", "source": "training_data_references = [\n                {\n                    \"id\": \"attrition\",\n                    \"type\": \"container\",\n                    \"connection\": {},\n                    \"location\": {\n                        \"path\": \"modeling_records_2022.csv\"\n                    },\n\n                    #\"type\": \"s3\",\n                    #\"connection\": {\n                    #    \"access_key_id\": \"yqcPbWZ0AQPHleHVerrR4Wx5e9pymBdMgydbEra5zCif\",\n                    #    \"endpoint_url\": \"https://s3.us.cloud-object-storage.appdomain.cloud\",\n                    #    \"resource_instance_id\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/7d8b3c34272c0980d973d3e40be9e9d2:2883ef10-23f1-4592-8582-2f2ef4973639::\"\n                    #},\n                    #\"location\": {\n                    #    \"bucket\": \"faststartlab-donotdelete-pr-nhfd4jnhlxgpc7\",\n                    #    \"path\": \"modeling_records_2022.csv\"\n                    #},\n                    \"schema\": {\n                        \"id\": \"training_schema\",\n                        \"fields\": [\n                            {\"name\": \"POSITION_CODE\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"DEPARTMENT_CODE\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"DAYS_WITH_COMPANY\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"COMMUTE_TIME\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"AGE_BEGIN_PERIOD\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"GENDER_CODE\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"PERIOD_TOTAL_DAYS\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"STARTING_SALARY\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"ENDING_SALARY\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"NB_INCREASES\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"BONUS\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"NB_BONUS\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"VACATION_DAYS_TAKEN\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"SICK_DAYS_TAKEN\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"PROMOTIONS\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"NB_MANAGERS\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"DAYS_IN_POSITION\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"DAYS_SINCE_LAST_RAISE\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"RANKING_CODE\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"OVERTIME\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"DBLOVERTIME\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"},\n                            {\"name\": \"TRAVEL\", \"nullable\": True, \"metadata\": {}, \"type\": \"double\"}\n                        ]\n                    }\n                }\n            ]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The cell below will authenticate with the IBM FactSheet service using credentials you have already supplied and initialize FactSheet monitoring for this model. Note that Python notebooks in Watson Studio have full support for `pip install`, which allows you to add whatever libraries you need to the notebook environment. For example, if you wanted to use Python to parse command line arguments, you could run `!pip install argparse`.\n\nIf you receive an error related to the project access token, it is likely because you either did not insert the project access token as instructed at the beginning of the notebook, or did not run the cell after it was inserted. You will need to return to the beginning of the notebook, ensure the cell is inserted, and execute it."}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from ibm_aigov_facts_client import AIGovFactsClient\nexcept:\n    !pip install -U ibm-aigov-facts-client\n    from ibm_aigov_facts_client import AIGovFactsClient\n        \nPROJECT_UID= os.environ['PROJECT_ID']\nCPD_URL=os.environ['RUNTIME_ENV_APSX_URL'][len('https://api.'):]\nCONTAINER_ID=PROJECT_ID\nCONTAINER_TYPE='project'\nEXPERIMENT_NAME='predictive_attrition'\n\nPROJECT_ACCESS_TOKEN=project.project_context.accessToken.replace('Bearer ','')\n\nfacts_client = AIGovFactsClient(api_key=API_KEY,experiment_name=EXPERIMENT_NAME,container_type=CONTAINER_TYPE,container_id=CONTAINER_ID,set_as_current_experiment=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next two cells construct metadata for your model. This will be saved with the model itself and will appear on its FactSheet. If you get errors trying to save the model, they will most likely be from the metadata contained in the model properties, specifically the `TYPE` and `SOFTWARE_SPEC_UID`, which frequently change as Watson Studio adds support for new versions of Python, and removes support for outdated versions. You can get a list of current supported specifications by running `wml_client.software_specifications.list()`."}, {"metadata": {}, "cell_type": "code", "source": "fields=X_train.columns.tolist()\nmetadata_dict = {'target_col' : 'ATTRITION', 'fields':fields}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"runtime-22.1-py3.9\")\nprint(\"Software Specification ID: {}\".format(software_spec_uid))\nmodel_props = {\n    wml_client._models.ConfigurationMetaNames.NAME:\"{}\".format(\"attrition challenger - sklearn\"),\n    wml_client._models.ConfigurationMetaNames.TYPE: \"scikit-learn_1.0\",\n    wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n    wml_client._models.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n    wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"ATTRITION\",\n    wml_client._models.ConfigurationMetaNames.CUSTOM: metadata_dict\n}\n\nfacts_client.export_facts.prepare_model_meta(wml_client=wml_client,meta_props=model_props)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next three cells fit the data the the model using a Random Forest classifier, run predictions on the test data, and then print out the accuracy for how the model did on the test data. Finally, the notebook calculates and displays feature importance. For more information on Random Forest classifiers, see [here](https://www.ibm.com/cloud/learn/random-forest)."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf = RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred = clf.predict(X_test)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "feature_imp = pd.Series(clf.feature_importances_,index=X.columns).sort_values(ascending=False)\nfeature_imp", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The next three cells export data from the model you just created to the FactSheet. The first lists experiments tracked by FactSheets. The second writes the URL and other info on this notebook as custom data to the FactSheet. Note that any data can be written to the FactSheet that might be helpful for model validators."}, {"metadata": {}, "cell_type": "code", "source": "facts_client.runs.list_runs_by_experiment('1')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "nb_name = \"attrition model creation and deployment\"\nnb_asset_id = \"tbd\"\nnb_asset_url = \"https://\" + CPD_URL + \"/analytics/notebooks/v2/\" + nb_asset_id + \"?projectid=\" + PROJECT_UID + \"&context=cpdaas\"\n\nlatestRunId = facts_client.runs.list_runs_by_experiment('1').sort_values('start_time').iloc[-1]['run_id']\nfacts_client.runs.set_tags(latestRunId, {\"Notebook name\": nb_name, \"Notebook id\": nb_asset_id, \"Notebook URL\" : nb_asset_url})\nfacts_client.export_facts.export_payload(latestRunId)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "RUN_ID=facts_client.runs.get_current_run_id()\nfacts_client.export_facts.export_payload(RUN_ID)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Finally, the model is stored to the project with all of the metadata defined above."}, {"metadata": {}, "cell_type": "code", "source": "print(\"Storing model...\")\npublished_model_details = wml_client.repository.store_model(\n    model=clf, \n    meta_props=model_props,\n    training_target=['ATTRITION'],\n    training_data=X)\nmodel_uid = wml_client.repository.get_model_id(published_model_details)\n\nprint(\"Done\")\nprint(\"Model ID: {}\".format(model_uid))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Next, the notebook uses Apache Spark to create a second model. Because you specified a Spark environment when you created this notebook, the `pyspark` runtime will be available without needing to be installed via `pip`."}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark.sql import SparkSession\nexcept:\n    print('Error: Spark runtime is missing. If you are using Watson Studio change the notebook runtime to Spark by clicking \\\n    the Vew notebook info button above (the lowercase i in a circle). Click on the Environment tab and use the Environment \\\n    definition dropdown to select an environment with Spark and Python.')\n    raise\nspark.version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# <span style=\"color:red\">3. !!--STOP--!! Insert data to code below</span>\n\nPlace your cursor in the empty code cell below. Then click the **Find and add data** icon in the upper right corner of the screen like you did in step 2. Locate the *modeling_records_2022.csv* file, click its associated **Insert to code** dropdown, and select **SparkSession DataFrame**.\n\n## <span style=\"color:red\">IMPORTANT: replace all instances of `df_data_x` with `df_data_2` in the code</span>\n\nThe automated dataframe will likely use the `df_data_3` variable to hold the data. Update the last two lines of code to import data into the `df_data_2` variable for the rest of the notebook to work correctly. The last lines of your cell should look like this:\n\n<img src=\"https://cp4d-outcomes.techzone.ibm.com/img/data-fabric-lab/trusted-ai/dataframe_insert_2.png\" width=700 align=left>"}, {"metadata": {}, "cell_type": "markdown", "source": "Run the inserted code cell below. If you have correctly imported the data, you will see a table populated with employee data. The remainder of the notebook is very similar to the training of the sklearn model. It will enable FactSheets for the second model, train a Spark Gradient Boost Classifier, and then save that model to the project. You may run the rest of the notebook to its conclusion."}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Similar to the `sklearn` model, you need to specify metadata for the spark model."}, {"metadata": {}, "cell_type": "code", "source": "software_spec_uid = wml_client.software_specifications.get_id_by_name(\"spark-mllib_3.2\")\nprint(\"Software Specification ID: {}\".format(software_spec_uid))\nmodel_props = {\n    wml_client._models.ConfigurationMetaNames.NAME:\"{}\".format(\"attrition challenger - spark\"),\n    wml_client._models.ConfigurationMetaNames.TYPE: \"mllib_3.2\",\n    wml_client._models.ConfigurationMetaNames.SOFTWARE_SPEC_UID: software_spec_uid,\n    wml_client._models.ConfigurationMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n    wml_client._models.ConfigurationMetaNames.LABEL_FIELD: \"ATTRITION\"\n}\n\nfacts_client.export_facts.prepare_model_meta(wml_client=wml_client,meta_props=model_props)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.classification import GBTClassifier\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml import Pipeline, Model", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "For the second model, you will create a Gradient Boosted Tree classifier. For more information on Gradient Boosting, see [here](https://www.ibm.com/cloud/learn/boosting)."}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.sql.types import FloatType\nfor field in fields:\n    df_data_2=df_data_2.withColumn(field,df_data_2[field].cast(\"float\").alias(field))\ndf_data_2=df_data_2.withColumn('ATTRITION',df_data_2['ATTRITION'].cast(\"int\").alias('ATTRITTION'))\ndf_data_2.take(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "va = VectorAssembler(inputCols = fields, outputCol='features')\nva_df = va.transform(df_data_2)\nva_df = va_df.select(['features', 'ATTRITION'])\nva_df.show(3)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "gbtc = GBTClassifier(labelCol=\"ATTRITION\", maxIter=20)\n\npipeline = Pipeline(stages=[va, gbtc])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "split_data = df_data_2.randomSplit([0.8, 0.2], 24)\ntrain_data = split_data[0]\ntest_data = split_data[1]\n\nprint(\"Number of training records: \" + str(train_data.count()))\nprint(\"Number of testing records : \" + str(test_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "spark_model = pipeline.fit(train_data)\n\npred = spark_model.transform(test_data)\npred.show(3) ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "evaluator = BinaryClassificationEvaluator()\nevaluator.setLabelCol(\"ATTRITION\")\nprint(\"Test Area Under ROC: \" + str(evaluator.evaluate(pred, {evaluator.metricName: \"areaUnderROC\"})))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Storing spark model...\")\npublished_model_details = wml_client.repository.store_model(\n    model=spark_model, \n    meta_props=model_props,\n    training_target=['ATTRITION'],\n    training_data=train_data,\n    pipeline=pipeline\n)\nmodel_uid = wml_client.repository.get_model_id(published_model_details)\n\nprint(\"Done\")\nprint(\"Model ID: {}\".format(model_uid))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Congratulations!\n\nYou have completed this notebook. You can now return to the [Data and AI Live Demos lab page](https://cp4d-outcomes.techzone.ibm.com/data-fabric-lab/trusted-ai) and continue with the lab."}], "metadata": {"kernelspec": {"name": "python39", "display_name": "Python 3.9 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.9.7", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}